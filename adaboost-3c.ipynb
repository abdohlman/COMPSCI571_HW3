{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "* Go through an implementation of decision trees.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "*This file is adapted from course material by Carlos Guestrin and Emily Fox.*\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## please make sure that the packages are updated to the newest version. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a subset of the [LendingClub](https://www.kaggle.com/wendykan/lending-club-loan-data) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('loan_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding the target column\n",
    "\n",
    "We re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan. In the next cell, the features are also briefly explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "\n",
    "loans['safe_loans'] = loans['loan_status'].apply(lambda x : +1 if x=='Fully Paid' else -1)\n",
    "\n",
    "## please update pandas to the newest version in order to execute the following line\n",
    "loans.drop(columns=['loan_status'], inplace=True)\n",
    "\n",
    "target = 'safe_loans' # this variable will be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = pd.get_dummies(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_NONE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(loans.columns)\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(loans, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data weights change as we build an AdaBoost model, we need to first code a decision tree that supports weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    if weighted_mistakes_all_negative < weighted_mistakes_all_positive:\n",
    "        return (weighted_mistakes_all_negative,-1)\n",
    "    else:\n",
    "        return (weighted_mistakes_all_positive,+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = pd.Series([-1, -1, 1, 1, 1])\n",
    "example_data_weights = pd.Series([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function takes the data, the features, the targets and the data weights as input and returns the best feature to split on.\n",
    "  \n",
    "Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "               \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "        \n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "#         labels = data[target]\n",
    "        left_labels = data[target][data[feature] == 0]\n",
    "        right_labels = data[target][data[feature] == 1]\n",
    "                \n",
    "        left_mistakes_weight, left_class = intermediate_node_weighted_mistakes(left_labels, left_data_weights)\n",
    "        right_mistakes_weight, right_class = intermediate_node_weighted_mistakes(right_labels, right_data_weights)\n",
    "                \n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        error = (left_mistakes_weight + right_mistakes_weight)/float(sum(left_data_weights) + sum(right_data_weights))\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term_ 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. Relationship between weighted error and weight of mistakes:\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}] + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}({\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}({\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}({\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}({\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. A decision tree will be represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    \n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    leaf['prediction'] = best_class\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print \"Stopping condition 1 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees    \n",
    "    left_tree = weighted_decision_tree_create(left_split, remaining_features, target, left_data_weights,\n",
    "                                              current_depth=current_depth + 1, max_depth=max_depth)\n",
    "    \n",
    "    right_tree = weighted_decision_tree_create(right_split, remaining_features, target, right_data_weights,\n",
    "                                               current_depth=current_depth + 1, max_depth=max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Split on feature grade_A. (8775, 75)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8775 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (75 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Split on feature grade_D. (19331, 3819)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19331 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3819 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found:', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there: 7' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_D'},\n",
       " 'splitting_feature': 'term_ 36 months'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (a dataframe)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data \n",
    "    prediction = data.apply(lambda x: classify(tree,x), axis=1)\n",
    "\n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39087499999999997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Split on feature grade_A. (19673, 2740)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19673 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2740 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14999999999999999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44562499999999999"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture notes the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data (uniformly weighted)\n",
    "    alpha = np.array([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in xrange(num_tree_stumps):\n",
    "        print '====================================================='\n",
    "        print 'Adaboost Iteration %d' % t\n",
    "        print '====================================================='        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "#         tree = weighted_decision_tree_create(data, features, target, alpha/float(len(data)), max_depth=1)\n",
    "        tree = weighted_decision_tree_create(data, features, target, alpha/float(len(data)), max_depth=1)\n",
    "        tree_stumps.append(tree)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree,x), axis=1)\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        weighted_error = sum(alpha[is_wrong])/float(sum(alpha))\n",
    "                \n",
    "        # Compute model coefficient using weighted error\n",
    "        weight = 0.5*np.log((1.0 - weighted_error)/weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))        \n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        alpha *= adjustment\n",
    "        alpha /= sum(alpha)\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('_')\n",
    "    print '                       root'\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name)))\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]            [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17198848113764034, 0.17728780637269631]\n"
     ]
    }
   ],
   "source": [
    "print stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.17, 0.18]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.array([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis = 1)\n",
    "        \n",
    "        # Accumulate predictions on scores array\n",
    "        scores += stump_weights[i]*predictions\n",
    "    \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.62825\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_data[target], predictions)\n",
    "print 'Accuracy of 10-component ensemble = %s' % accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17198848113764034,\n",
       " 0.17728780637269631,\n",
       " 0.10308067697010909,\n",
       " 0.086867020583368509,\n",
       " 0.072200859377939741,\n",
       " 0.07438562925258671,\n",
       " 0.058345528732444689,\n",
       " 0.045454870264750973,\n",
       " 0.031945484600118701,\n",
       " 0.023305292432239024]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** i: Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "The stump weights are monotonically decreasing.\n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some performance plots.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_C. (23388, 8612)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23388 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8612 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_2 years. (29104, 2896)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29104 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2896 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_OWN. (29204, 2796)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29204 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2796 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.41484375\n",
      "Iteration 2, training error = 0.4328125\n",
      "Iteration 3, training error = 0.39059375\n",
      "Iteration 4, training error = 0.39059375\n",
      "Iteration 5, training error = 0.3793125\n",
      "Iteration 6, training error = 0.38228125\n",
      "Iteration 7, training error = 0.37253125\n",
      "Iteration 8, training error = 0.3755\n",
      "Iteration 9, training error = 0.37253125\n",
      "Iteration 10, training error = 0.37253125\n",
      "Iteration 11, training error = 0.37253125\n",
      "Iteration 12, training error = 0.3715\n",
      "Iteration 13, training error = 0.37253125\n",
      "Iteration 14, training error = 0.3715\n",
      "Iteration 15, training error = 0.3715\n",
      "Iteration 16, training error = 0.3715\n",
      "Iteration 17, training error = 0.3715\n",
      "Iteration 18, training error = 0.37146875\n",
      "Iteration 19, training error = 0.3715\n",
      "Iteration 20, training error = 0.37146875\n",
      "Iteration 21, training error = 0.37209375\n",
      "Iteration 22, training error = 0.37146875\n",
      "Iteration 23, training error = 0.372125\n",
      "Iteration 24, training error = 0.3715\n",
      "Iteration 25, training error = 0.3715\n",
      "Iteration 26, training error = 0.372125\n",
      "Iteration 27, training error = 0.3715\n",
      "Iteration 28, training error = 0.3713125\n",
      "Iteration 29, training error = 0.37121875\n",
      "Iteration 30, training error = 0.37125\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - accuracy_score(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdYAAAFcCAYAAAB1MZ/kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3LA6juM4ToL8PzUbzXQOKWkGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112784e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.410375\n",
      "Iteration 2, test error = 0.43175\n",
      "Iteration 3, test error = 0.390875\n",
      "Iteration 4, test error = 0.390875\n",
      "Iteration 5, test error = 0.37825\n",
      "Iteration 6, test error = 0.382625\n",
      "Iteration 7, test error = 0.37175\n",
      "Iteration 8, test error = 0.376125\n",
      "Iteration 9, test error = 0.37175\n",
      "Iteration 10, test error = 0.37175\n",
      "Iteration 11, test error = 0.37175\n",
      "Iteration 12, test error = 0.369375\n",
      "Iteration 13, test error = 0.369375\n",
      "Iteration 14, test error = 0.369375\n",
      "Iteration 15, test error = 0.369375\n",
      "Iteration 16, test error = 0.369375\n",
      "Iteration 17, test error = 0.369375\n",
      "Iteration 18, test error = 0.371\n",
      "Iteration 19, test error = 0.369375\n",
      "Iteration 20, test error = 0.371\n",
      "Iteration 21, test error = 0.36925\n",
      "Iteration 22, test error = 0.371\n",
      "Iteration 23, test error = 0.367625\n",
      "Iteration 24, test error = 0.369375\n",
      "Iteration 25, test error = 0.369375\n",
      "Iteration 26, test error = 0.367625\n",
      "Iteration 27, test error = 0.369375\n",
      "Iteration 28, test error = 0.3695\n",
      "Iteration 29, test error = 0.369\n",
      "Iteration 30, test error = 0.369\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - accuracy_score(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print \"Iteration %s, test error = %s\" % (n, test_error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFTCAYAAAAKvWRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XecFPX9+PHX+3rj4I4iSJUqYkVUjDXYSzRiFDuWr1iT\nGDXYUBGNJkGjsUSjxm6iUbHElvwQ0GhsiIoURRCQ3u64447r9/798Zm9293b3Zure7e8n4/HPm7n\nM5+Z+ezs7r13Zj6f94iqYowxxpj4SYp3A4wxxpgdnQVjY4wxJs4sGBtjjDFxZsHYGGOMiTMLxsYY\nY0ycWTA2xhhj4syCsYlIRCaKyHwRKRURFZGr4t0mE1u83zNvm3PivQ7TOYjIHBHxPbZWRJ7yPh+D\n2q5V8WPBuAMSkUHehy74USEiK0TkCREZ0sbbPxh4CkgHHgBuAz5py22almnpeyYiv/U+Z1Ui0rtt\nWpmYEj1ImPaREu8GmJi+A17wnucChwMXAKeIyP6q+n0bbfc47+9EVbUg3Dm09D27AFDc/4Rzgemt\n1TBjTOPsyLhj+1ZVp3qPq4F9gaeBbsBNbbjdPt7f9W24DdO6mv2eichYYCTwBLAVF5iNMe3IgnEn\noi536V+8yTHB80Skt4jcLyI/eKe0N4jIcyKyS/h6AtflRGSAiPxdRDZ5Zed713AC/4yXB06Thy1/\nkYh87l2bLBaRD0Tk5Ajbmeotf7iI/J93PbNcRJ6KMP8iEVkoImUi8p2InOvVSRORO0Rkpbfs517w\nCN/WOBF5UkSWeO3aJiL/E5EJEeoGLgM8JSJDReQ1ESnylvlXtMsAIjJaRF4UkXXePl7tLXtIWL10\n77Tv1yKy3Vv3TBE5LNJ6o/Ha+ZS3vUpvH9wvIj2C6hzu5z1rRGDZvwEvASMj7eOgbZ4mIl9678ca\nEblHRDKj1N1XRB7y3tti772ZJyJXiIg08tpfEZFCESkRkX+LyF5R6h4qIu96dctE5BsRuVZEGpz5\nE5FUb943Xt1Cb9lDItTNE5E7ReRb730sFJEFIvIXEcnx6qwAJnqL1O37wGe8MSKyj4i8JO77WiEi\ny0TkrsD6g+od7q13qojs532eSkSkQESeF5GeEdZ9pIj8PxFZ771Xq739eFKEuj8VkbdFZItXd5GI\nXB++D8X7H+H9PVlE5nr7ZqWIXOPVERG5Wtx3sdx770+MsQ8yReRe77NU7n22TvOz/7zlk0TkYhH5\n1NsnJeK+++P9rqNDUFV7dLAHMAh3yvC1CPMO8OYtCCobBqwBaoA3cacYXwAqgU3AkLB1KPANsAr4\nHPgT7qjoJ8BU4Cuvzn3e9NSgZR/w5q3wlnsQ2OCVXR22nale+TvANuB54A/Ab8Lmvw5s8drwkPdc\ngeO9eUu87T4LVAOFQNewbb3r1XsW+D3wKO4oUYGrouzfOcBm4D3gbuDfXvkPQGbYMhO8/Vnu7du7\ngCeBpcB9QfUygA+89Xzm7cPHgI1e28f7/AyM8N67WmCGt71A+5YBPYNeS8z3rJHtZOKOhr/3pg/x\n1vNolPoXevMLcD8M7wGW4z53CswJq/8IsBr4O/BH4GGv/Rq838I+m18DPwL/81733719VwzsGVb/\n9KB5j3nbWOCt5zVAguqK93lSYKFX9zFv2WrgtLC6n3n7/12v7n3Av4DtQD+v3lWR9j3wcx/7/hSg\nAvfdeBb3vX3PW9cnQFpQ3cO98re87b+B+8x+7JV/HPZaT/Tavhb4K/Wf10XA42HtuNKruxH3HbwH\n+NRb74ywuud75W8ApcBz3uv+0Su/DLjfe88fwX0Pt+O+O0PD1jXHW+ZN3HfuHtxnqsArvzCs/lNe\n+aCw9+nFoPf0Ie+xwiv7dbz/n/t9xL0B9ojwpkQJxt4HL/CBfDKo/GPvS31oWP0DgSrgzbBy9R6P\nBH+Bg+Y3+NB75YdR/88yJ6i8N+7HQBUwOKh8qle/CNg1wnYC8zcBA4PK9/XKC4H3CQqMwDVEDvy7\nRFh/ttfWIiArwv5V4JqwZZ70ys8Me32lXntGRnhPdg6avstb/vqwej29fxCbCAv0UT4Ds731nBtW\nfotX/oSf98zHds7xlrs16PUsD99n3ryuuMBVFLy/gS64f4SRgvEAICmsLAUX4GqC3/ewz2b46xvv\nlX8QVJaL+yFREvy+eOsP/HA5L6h8olf2byAlqHyk9/5uBbp4ZXt6df8UYZ/lEhoom7zvgR7evvwh\n+PPjzfutt75rg8oOD9o3pwaVJ1EfwA8MKp+B+5/QM8K2uwc9H4X73n5C0A9c73PwoLfeXwSVn++V\nVQD7BJX3xf1Q3YoL+MHbONVb5v6wdszxyucD2cHfZe8zVhzWpgb7GbjEK3sISA777n/qtXPn8H3Q\nER9xb4A9Irwp9cHiW+p/af8J+IL6o5JhXt3RXtmDUdb1Mu6fXvCHWr0vTn6UZSL+c8H9albgpAjL\nXOXNuzmobKpXNj3KdqaGLxM0b6k375Cw8n5e+dM+9+XVXv3DI+zfZTQMFIEfHPcElV1HhAAbYVtJ\nuIC9IMr8K731nNjIegZ49eZFmJeBO+Ivo4UBwVtulrfckKCyO7yyc8LqnueV/zHCes4iQjCOsd1A\ncD0/rFxxwaFfhGU+8+YPCGtPpIC5tzfvvQivdc8I9e8j6McP9cH4dz5eS5P3fdDn8hcR5iXhjlLn\nBpUdHm3/Uv8j45dBZTNwP1K6NdKO+71lx0SYl4s7Yn45qOx8r/7fItSfGbwPw15PBfB+WPkcr/4Z\nEdb1Rxr+mGqwn3GBvCD4uxA070Sv/pVN+U7E62G9qTu2EcCt3vMq3CmnJ4A7VHW5V36A97efiEyN\nsI4+uC/DMGBuUPlyVS1oYnv29v7OiTBvTlidYHMjlAX7OkLZemBIhHmBDko7BxeKSC4wGTgZGAxk\nhS3Xh4bmq2ptWNka72+3oLL9vL//ibCOYCO85VZGeS+GeX93xZ2aiybqflbVchH5BPc6R+AuNzSL\nuKE4hwMfq+qyoFnP4joIXog7DRkQuGb73wir+zDKNtKBX+FO848AcsKqRHpfVqrq6ijb2M9rx4/E\n3k9fiUgRoZ/HvYFCVZ0fYd1zgF97dZ7FHd0tAG4Qkb1xp4f/i/uhpRGWb6rA9/ZgEdk9wvwq3Ock\n3JcRyiJ9Zl/EnQZfICIv4F7fh6q6NUI7FDgpynXdsijtiPadbTBPVWtFZBNh39kgkT47H+LOEETs\nJwAgIlnA7rjPwo0RuiAErqNHan+HY8G4Y3tdVX/eSJ187+/J3iOa7LDpjc1oTy5QrqrFEeatD6oT\nrrFtRVpfNUD4tlS12vvSpQbKRCQNdzp7b9zZg6dwv5ZrvLKTceNvwxVF2y6QHFTW1fu7NvbLqHsv\n9iLGPxEavhfhAvtwQ5T5sfZ1U5yPOx0ZHHBR1e9E5HPgcBHZJeiHX2A/RHo/o7X1FeAE3Fmev+NO\n01fjzk5MJPL7sinKugLbyA37G2s/BXfGywWiDQcM2afe52wcMA13FH+8N3+ViNyhqo9GWY9fgc/K\nr5u4nK/PrKq+KCLVuCPw3+Au71SLyL9wfSh+DGqHADfH2Gakz2vU72yMeakRyiHy+x3+XkeSh2v7\nQOoPWiJp7PvWIVgw7vwCH/yLVfXxJizXnF/3xcAQEcmNEJB3CmtPS7fVFCfjgu6jqnpJ8AwRuY7Y\nP1L8CBxN7EzsoUOB1/68qp7Tgu0F1rNTlPmx9rUvXk/mid7kQyLyUJSq51P/jy4QCHrFaFPwNvbD\nBeJ3gROCz0KI6+U+MXwZT4OewWHbKA77G2s/Be+j4kbqBq8TVd0EXCYiV+COwI7CXY75q4hsUtVX\no6zLj8B2hqnq0hasJypVfQV4RUTycB3zzgTOAAaLyD7eEX4x7kdrtqpWtEU7fOiJ6/AVzM9nPDDv\nI1U9uNVb1c5saFPn95n3N+pQlFb0lff30AjzDgur054CRz//ijDvoFZY/+fe36MbqbcY1zN2PxFJ\nbqRuLFH3s3fa9wDcNf/vWrCNcbij029xQ5oiPaqBiVJ//i9w+rHBMCAg0j/DwPvyVoTLAbHel4Ei\n0i9CeWCZQDti7ac9cadtgz+PXwF5UU4LR/38qmqtqs5X1XtwAQ0geHhQjfe3Ke95u31vVbVQVd9Q\n1TNxnb32wvVLCLQjmfpLMfEQ6bMTKIt0OhwAVd2G+/zuHj4UrDOyYNzJqeqnuC/UBSLys/D53rjK\n1vrV+Iz39zbvek1gG71w12urcaci21vglFvIP3hvnGGDfdIMz+CGZ1wnIiPDtiEi0gfcqU1cD/Xh\nwB2RArKIHBC87yLxTiG+D+wrDcdJX4u7zvqCqlY29wXhrgcD3KKq/xfpgRuSNhA4wqv7Bu7HxiQJ\nGr/u/SOMlIQm2vsyFpgUo20puHSewcuMB/YH/ht0ivV13NHRJBEZGlQ3GTeEDuo/s8HP7wp+b0Rk\nuNeeIm+diMguIhLpWmPgiK0sqCzQ96JvjNcU7klcB6s/iMiw8Jki0k1E9mnC+sKXH+f9cAsuS6H+\n9Hi59/cvuB8TDwU+x2HL7BT+mW8DN4pI3alk77N1Ce6z9nojyz6Au3zyFxHJCJ8pIqO8/08dnp2m\nTgxn4YbCvCEi/8X9uq/G/SM9BPfPosWdGFR1jog8jBtLuEBEXgXScGM9ewGTwzoCtZd/4f7xXyci\no3C/lkcBxwKv4jqyNJuqrheRQGemL73XvRz3mg8F3sadvgQ39GgMcD0ubel/cfu/n1c+HBdMtzey\n2ctwnVj+7iVAWILrOX+Mt+3rmvt6RKQrbp8U4AJsNE/ifsxcAMxU1a3ibj7xN+ALr2NQGe6a6kJg\nt7DlP8V13jtDXL7rz3Gd607ytntqlO3OB44WkY9wY7YHAqfhgteVgUqqWiQil+Lel0B7tuKu7+6O\n6yQXHox/getl+6WIvIO77jgBN9767KDLL3sBr3qd5RbirpPvAvwc9949ErTe2bgfSY+IyMve/G9U\n9a0orw9V3SgiZ+M6Wi0Ukbdx17OzvX10GC7b3qXR1tGIP+E6dc7BDalLBo7E7ZfnVHWD145vROSX\nuGFMS7x2rPD2yzDcEerNuLM+beVH4Bvve5WBO/uQC1ykqpGukQd7GJcf4VxcH4dZuEtJfXA94vfG\nDfFsTh+Z9hXv7tz2aPggRtKPGMt0x41xXYT7B1mM+wL9DTgirG7MISjEGKqB6zBxMa6j1HbcP8j/\nEiGZBfVDlw6Psp2o8/GGPURZrkH7cadEX8V1BtnmtekY6odinB9h/z4VY99HmrcfrkPSJtxQjVXe\n9EFh9VKAK3BjN4u99+MHXBKK8wga49rIezoYF0DW45Im/Ij7p9mrKe9ZhLqBsZkRh8MF1UvF/RMr\nI2iIDO7H11e4o6s1uGQNmVHel528tq31Pi9fAGdTP1RnaqT31nsfZuCGipXierLvHaWdh+PGDm/1\n2rQQ92MlNcprus6rExgX+2/gsLB6/XDJYz719kG59x4+TdhYc6/+DbihclXRPj9R2r6bt39Wee/x\nZmCet+1dw15jg/0VbR7uB8Y/vTZtxyXS+cx77xt8/nAB6yVgndeO9d7n9xa8oWRevfOJMCTNx/+N\nFcCKSN9x3MiHe73PSLn32Tqties/G/ejqBD33fzRe18vI2gMc0d+iPdCjDHGGBMnds3YGGOMiTML\nxsYYY0ycWTA2xhhj4syCsTHGGBNnNrQpgh49euigQYPi3QxjjDGd3BdffLFZVaNllatjwTiCQYMG\nMXduY/c2MMYYY2ITkZV+6tlpamOMMSbOLBgbY4wxcWbB2BhjjIkzC8bGGGNMnFkwNsYYY+LMgrEx\nxhgTZza0yRiTUIqLi9m4cSNVVVXxbopJcKmpqfTq1Yvc3NwWr8uCsTEmYRQXF7Nhwwb69u1LZmYm\nIhLvJpkEpaqUlZWxZs0agBYHZDtN3UGsLtzOtH8t4oH3vqe8qibezTGmU9q4cSN9+/YlKyvLArFp\nUyJCVlYWffv2ZePGjS1enx0ZdwA1tcrEJz5j2aZSANYWlXHX+D3j3CpjOp+qqioyMzPj3QyzA8nM\nzGyVSyJ2ZNwBLF5XXBeIAWZ92/JfWcbsqOyI2LSn1vq8WTDuABauLQqZLiitRFXj1BpjjDHtzU5T\ndwAL1xazq/zI5Smvs1VzuKf6NIrLq+mamRrvphljjGkHdmTcAXy7poDH0+7mpOSPOS/l/zEt9SkK\nSivj3SxjTDsTkUYfc+bMafF2evfuzZQpU5q0THl5OSLC448/3uLtm4bsyDjOamqVmvUL6Je0ua7s\nJ0kL+LG0kl16ZMexZcaY9vbxxx/XPS8rK2PcuHFMmTKFE044oa58t912a/F23n77bXr16tWkZdLT\n0/n4448ZMmRIi7dvGrJgHGcrtpTSr3oVpNWX5VHCVyXl8WuUMSYuxo4dW/e8pKQEgCFDhoSUR1Ne\nXk5GRoav7YwePbrJbRMRX+2IN1WlsrKS9PT0BvPKysqa3du+srKSlJQUkpLa5oRyu5+mFpH+IvKy\niBSJSLGIzBCRAc1Yzw0ioiLyYVh5FxH5p4gsFZFSEdkqIp+KyDmt9ypaz8K1xQxLWhNSliK1lGzd\nFKcWGWM6ukceeQQRYd68eRxyyCFkZmbywAMPoKpcc8017L777mRnZ9O/f38mTpzIpk2h/0/CT1Of\nccYZHHzwwbz99tuMGjWKnJwcDjvsML777ru6OpFOU48dO5ZzzjmHp59+msGDB5Obm8vPfvYz1q9f\nH7K9H374gaOOOorMzEyGDBnC3//+d0488USOPfbYRl/ryy+/zOjRo8nIyGDnnXfmpptuoqamPhfD\n9ddfT79+/Zg9ezajR48mPT2dN954g3fffRcRYdasWRx//PFkZ2dz7bXXAu6HzuWXX06vXr3IzMzk\ngAMOYPbs2SHbDby2Bx98kF122YXMzEy2bNni491pnnY9MhaRLGAWUAFMBBS4A5gtInuqamms5YPW\nMxi4CYg0BigNqAbuAlYA6cAE4FkR6amq97b0dbSmhWuK2EfWNigv37ohDq0xJrEMuv6teDcBgBW/\nP6HxSs0wYcIErrjiCqZNm0Z+fj61tbUUFBQwZcoU+vTpw4YNG5g+fTpHH3008+bNizkMZ+nSpUyZ\nMoWpU6eSmprK1VdfzZlnnsm8efNituGDDz7gxx9/5L777qO4uJirrrqKyy+/nBkzZgBQW1vLiSee\nSGVlJU899RQpKSncdtttFBQUsPvuu8dc9zPPPMMFF1zAlVdeye9//3u+++47brzxRkSEO+64o65e\nUVER//d//8cNN9zA4MGDGTBgAEuXLgXg/PPP56KLLuLaa68lKysLgIkTJzJz5kzuuusuBg0axMMP\nP8wxxxzDhx9+yP7771+33vfee48lS5Zwzz33kJaWVrd8W2jv09QXA4OBEaq6FEBE5gPfA5cAf/K5\nnoeB54ERhL0GVd0CnBVW/20RGQ5cCHSsYLy2mNNkTYPyqmIba2yMie3aa6/lkksuCSl78skn657X\n1NSw7777MnToUD7//POQQBOuoKCATz/9lIEDBwLuSPjMM89kxYoVDBo0KOpypaWlvPXWW3Tp0gWA\n1atXM2XKFKqrq0lJSeHVV19l8eLFfP311+y5p0tmNHr0aIYOHRozGNfU1HDdddcxadIk/vznPwNw\n9NFHk5yczOTJk5k8eXJdCsqSkhJefvlljjnmmLrlA8H47LPP5tZbb60r/+qrr5gxYwYvvPACEyZM\nAOCYY45h11135Xe/+x2vv/56Xd1t27bxzjvv0L1796jtbC3tfZr6JOCTQCAGUNXlwEfAyX5WICJn\nAaOBG5q47S1Ah8ocr6p8t2YzA6XhUXBtiZ2mNsbEFtyxK+CNN95g7NixdO3alZSUFIYOHQrAkiVL\nYq5r+PDhdYEY6juKrV69OuZyBx54YF0gDixXU1NTd6r6888/Z9CgQXWBGGCXXXZhjz32iLneBQsW\nsH79ek477TSqq6vrHuPGjaO0tJTFixfX1U1NTeWoo46KuJ7wffTZZ5+RnJzM+PHj68qSk5P5xS9+\nwYcfhlz1ZOzYse0SiKH9g/EoYEGE8oVAo10ERSQPd2Q7WVULGqkrIpIiIt1FZBJwDHBfM9rcZtYV\nldO1fDWp0jAXtWzfHGEJY4ypt9NOO4VMf/TRR5xyyikMGTKE5557jo8//pgPPvgAcEe6sXTr1i1k\nOi0trVWWW79+PT179mywXKSyYJs3u/+BRxxxBKmpqXWPkSNHArBq1aqQdUXrWBW+j9atW0deXh6p\nqakN6hUWFsZcti2192nqfKAwQnkBkOdj+enAEuApH3WvAB7wnlcBv1bVZ6JV9gL2JIABA5rcn6xZ\nFq4tZmiE68UAKeUxf2sYY3xoq2u1HUX4NeBXXnmFAQMG8Pzzz9eVBXfCiofevXvz/vvvNyjftGkT\nvXv3jrpcfn4+AE8//XTE4VzBQ6xiXQsPn9enTx8KCwupqqoKCcgbNmwgLy8v5rJtKR5JPyLleWz0\nFYvIIcB5wGXqL1fki8B+wHHA48ADInJJtMqq+qiqjlHVMY39YmstC9cWMTTC9WKAjMq267VnjElM\nZWVldUemAcGBOR72228/VqxYwfz58+vKli9fzjfffBNzuT322IOePXuycuVKxowZ0+ARHjj92n//\n/ampqeHVV1+tK6upqeGVV17h4IMPbtY6W0N7HxkX4o6Ow+UR+Yg52F+BvwGrRSRwXiQFSPamy1S1\nIlBZVTcBgQuv73o9ue8WkSdUtUNcO16wppifJUUOxllVW9u5NcaYzu6oo47ikUce4be//S3HHnss\nH3zwAS+88EJc23TKKaew6667Mn78eO68805SUlKYOnUqvXv3jjlmNyUlhenTp3PxxRdTUFDA0Ucf\nTUpKCsuWLePVV1/l7bffJjk5ucnt2XvvvRk/fjyTJk2ioKCAgQMH8vDDD7NixYq4/nBp7yPjhbjr\nxuF2AxY1suxI4FJc0A48DgLGes8va2T5uUAO0H4XARqxKMaRcR5FbK+sbucWGWM6s/Hjx3P77bfz\n/PPPc9JJJ/Hpp5/y2muvxbVNSUlJvPXWWwwaNIjzzjuPq6++mt/85jcMGTKkrjd0NBMnTuSVV17h\n008/5dRTT+XUU0/l0UcfZezYsS1KvvH0009z5plncvPNN3PKKaewYcMG3n33Xfbbb79mr7OlpD3v\nDiQiVwF3A8NV9QevbBBuaNP1qnpPjGUPj1B8H5AM/BJYqqpRu/2JyEvAsUB3VY2Z+HnMmDE6d+7c\nmK+lpQpLK9n39n+zKP0CMqThgfr3tX3JuGou/fPbblybMYlm8eLFdR18TMe1ZcsWBg8ezPXXX88N\nNzR1YEzHE+tzJyJfqOqYxtbR3qepHwOuBF4XkSm468e3A6twp6EBEJGBwDJgmqpOA1DVOeErE5Gt\nQErwPO+68FhgJrAa6A6cDvwCF/A7xB0YFq4tpq9sihiIAfKlmNWllRaMjTGd3oMPPkhGRgZDhw6t\nS0QC7sjXOO0ajFW1VETG4YYnPYvruPUecJWqlgRVFdwRb3POQ3yDG7N8N+769GZgMXCiqnaMdDwE\nOm9F7kkNLj/1/JIyoFvUOsYY0xmkpaUxffp0fvzxR5KTkznggAN477332HnnnePdtA6j3W8Uoao/\nAqc2UmcFPnpYq+rhEcr+BxzfzOa1GzesKfL1YoAkUUoLNwJ92q9RxhjTBiZNmsSkSZPi3YwOze5n\nHCcL1xYxLEYwBstPbYwxOwoLxnFQWlHND5tLGRo2rKk27O2w/NTGGLNjsGAcB9+uL0ZVG1wzLuwa\n2huvttSCsTHG7AgsGMfBwrXF9GIrubK9vjAth+35oUOwpdTyUxtjzI7AgnEcLFxT3OAUNT2GkdQl\nNB9JSrmlxDTGmB2BBeM4WLguQuatHiNI6xoajNMrG8sQaowxJhFYMG5nVTW1LFlf0nCMcc8RZHbr\nFVKUXWXB2JgdiYg0+pgzZ06rbGvRokVMnTqVkpKSxiubNtfu44x3dN9vKKGyppZhqWFHxj1HkJWa\nE1KUq0VUVNeQntL0ZOjGmM7n448/rnteVlbGuHHjmDJlCiecUH8ryEi3E2yORYsWcdttt3HppZeS\nk5PT+AKmTVkwbmcL1hYBRLhmPIKkmtBMnd0pprC0it5dLRgbsyMYO3Zs3fPAEeuQIUNCyjuT8vJy\nMjIyGpSXlZWRmZnZrHXW1NRQW1sbci/iRGCnqdvZorXFdKWEnlJUX5icBnmDIDv0Pso9pIgtpRUY\nY0y45cuXc9ppp9GtWzeys7M54YQTWLZsWd18VWXatGkMHjyYjIwMevfuzfHHH8+WLVt49913Oe20\n0wDo06cPIsKuu+4ac3uzZ8/m4IMPJjMzkx49enDZZZexfXv9iJBHHnkEEWHevHkccsghZGZm8sAD\nD/Dtt98iIvzzn//krLPOomvXrnXbrq6u5qabbqJ///6kp6ezxx578NJLL4Vs94wzzuDggw/mn//8\nJyNHjiQ9PZ2vvvqqtXZjh2FHxu1sYaTbJnYfCskpkJVPLUIS7k5aXWU7C4pLYeeucWipMQlgagf5\n7kwtarxOE2zcuJGDDjqIvn378vjjj5OWlsbvfvc7jj76aBYvXkxaWhqPPfYY99xzD3/84x8ZOXIk\nmzZtYubMmZSVlXHggQdy5513cuONN/LWW2+Rn58f80h11qxZHHPMMUyYMIGbbrqJDRs2cP3117Nt\n2zaee+65kLoTJkzgiiuuYNq0aeTn19++/qqrruL000/nlVdeISXFhZ7rrruOBx98kNtuu4199tmH\nF154gdNPP50ZM2Zwyimn1C27ZMkSbrnlFm655RZ69OhB//79W3V/dgQWjNtRba2yaG0xJyaFdd7q\nMdz9TUqmNDmXLjX1X9ySwg2AJVM3xtSbPn06tbW1zJw5k65d3Q+OAw88kF122YVnn32Wiy66iM8+\n+4wTTzyRSy65pG65U0+tvy3AsGHDABg9ejS9e/eOub3rrruOI488MiTw9urVi5/97GfceuutdesC\nuPbaa0O2+e233wJw2GGHcd9999WVb9iwgYceeohp06Zx3XXXAXDMMcewcuVKpk6dGhKMN2/ezPvv\nv5/Qt8eg+H6XAAAgAElEQVS009TtaGXBdkoraxoeGfesPz20PTUvZFZZkeWnNsaEmjlzJsceeyzZ\n2dlUV1dTXV1NXl4ee+21F4F7se+999689tprTJs2jblz51JbW9usbW3dupUvvviC008/vW5b1dXV\nHHbYYQDMmzcvpH5wZ7NY5V9//TUVFRV1p6wDJkyYwPz58ykuLq4rGzx4cEIHYrBg3K4Wep23Gtwg\noufwuqeVafkhs6otP7UxJszmzZt5+umnSU1NDXn873//Y9WqVQBcdtll3HrrrTz//PPst99+9O7d\nm9tuu63JQXnLli2oKhdeeGHItnJycqitra3bXsBOO+0UcT3h5evWrYtYHpguLCxsUJbI7DR1O1q4\n1v3Si9STOqAmswfU/yCkpmRTezTNmMTUytdqO4r8/HzGjh1bd3o3WOC0dXJyMpMnT2by5MmsXLmS\nZ555hltvvZWBAwdy/vnn+95WXp47W3fXXXdx5JFHNpjfr1+/kGmRyHe/DS/v08fdHnbjxo3ssssu\ndeUbNmwI2W6sdSYSC8btaMGaIjIpp58E5ZyWJNeBy6PZPUKWsfzUxphwRxxxBO+88w577rknaWlp\njdYfOHAgN998M48//jiLFi0CqFuuvLw85rL5+fnss88+fP/991x//fUtb7xnr732Ij09nZdeeonJ\nkyfXlf/zn/9kzz33JDc3t9W21RlYMG4nqq7z1pDwzFt5gyC1fhxeck7o8KZUy09tjAkzefJkXnjh\nBY444giuuOIK+vTpw/r165kzZw5HHnkkp556KhdccAF9+/Zl//33Jzc3l//85z+sWrWKn/70pwB1\nQ5n+8pe/cOqpp5KTk8OoUaMibm/69Okcd9xx1NbWMn78eLKzs1mxYgVvvvkm9957LwMHDmzya9hp\np5244ooruOWWWwAXnF988UVmzZrFjBkzmrlnOi8Lxu1kQ3EFW0orOaRBT+oRIZNpXUNTYqZVFrR1\n04wxnUzv3r359NNPuemmm/jVr35FcXExffr04dBDD2X33XcH4Cc/+QlPPPEEDz30EJWVlQwbNoyn\nnnqK4447DoDhw4dz55138vDDD3PPPfcwbNiwup7P4Y444ghmz57N1KlTOfvss6mtrWXgwIEcd9xx\ndO/evdmv4w9/+AMZGRncf//9bNy4kREjRvDiiy+G9KTeUYiqxrsNHc6YMWM00COxtby3eAMXPT2X\na1Ne5MqU1+tnHPRrOGpa3WTR3Jfp+uZFddNzGMPhU99r1bYYk6gWL16c8L1uTccT63MnIl+o6pjG\n1mG9qdtJoPNWw57UoVlvsvNDew3m1m6lptZ+MBljTCKzYNxOAsOaIt06MVhK2D2N89lG4fbQnNXG\nGGMSiwXjdrJwbTGpVDNQwpJ49BgWOh3Wm7q7FFNQasHYGGMSmQXjdrB1eyWrC8sYJOtJkaAB97l9\nISOs+35GN2qC3pYuUkZh8bZ2aqkxxph4aPdgLCL9ReRlESkSkWIRmSEiA5qxnhtEREXkw7Dy4SLy\nZxGZLyIlIrJORN4Qkb1a71U0zaJAso8Gp6iHN6yclMS25G4hRaUF69uqacYkHOuUatpTa33e2jUY\ni0gWMAvYFZgInAsMA2aLSHYT1jMYuAmIlCvyaOCnwNPAz4DLgZ7ApyKyb4teQDMtjBaMe46IUBvK\nUkODcdlWy09tjB+pqamUlZXFuxlmB1JWVtYq91Zu73HGFwODgRGquhRAROYD3wOXAH/yuZ6HgeeB\nETR8DS8AD2nQzxURmQWsAH4NnNeC9jdLXeet8DHGUYJxRVp3KP+hbrrK8lMb40uvXr1Ys2YNffv2\nJTMzc4dIo2jiQ1UpKytjzZo1rZI7u72D8UnAJ4FADKCqy0XkI+BkfARjETkLGA2cCTRI06KqDfJH\nqmqRiCwB+rag7c0WdVhTj8jBuCYzPyQ/de02C8bG+BFIobh27Vqqqqri3BqT6FJTU9lpp51aJXVn\no8FYRNKA9cD5qvpGC7c3Cng9QvlC4LQI5eFtyQPuBSaraoHfX70ikg/sDjzpv6mto6yyhmWbSkii\nlsHhqTCjHBlrdmhKTLZbSkxj/MrNzd3h8hqbzq/Ra8aqWglUA7GzifuTDxRGKC8A8iKUh5sOLAGe\nauJ2HwAEuC9aBRGZJCJzRWTupk2td6ekb9cXU6vQVzaRIUG/1LO6NxjGFJCcE5oSM6XcbhZhjDGJ\nzG8HrteAX7TSNiN1PWv0EFdEDsFd771Mm9B9TURuAM4Crgw+Pd6gUaqPquoYVR3Ts2fPaNWabEET\nT1EDpOWGbj/D8lMbY0xC83vN+B3gfhF5GReY1xEWVFV1lo/1FOKOjsPlEfmIOdhfgb8Bq0Uk0N04\nBUj2pstUtSJ4ARG5FLgTmKKqT/hoX6tbFC3zVs8Iw5o8md16h05XbW31dhljjOk4/AbjV7y/471H\ngOKOahVI9rGehbjrxuF2AxY1suxI73FphHmFwG8IOg0tIucCfwHuUdXf+Whbm6gf1hR+vXjXCLWd\nBvmpa7aiqtYz1BhjEpTfYPzTVtreG8DdIjJYVX8AEJFBwEFAY3etjtSG+3A/An4J1J2CFpFTcJ21\nHlfVa1ve7Oapqqnl2/Uue9awJB8JPzzpXUOPjPMpprismq5ZLR/LZowxpuPxFYxV9f1W2t5jwJXA\n6yIyBXdEfTuwCncaGgARGQgsA6ap6jSvDXPCVyYiW4GU4HkicijwD2A+8JSIjA1apEJVv2yl19Ko\nZZtKqKyuBZQhPhN+ABHzU68vrbBgbIwxCapJ44y9IUIH4q77bsGNGfbdu0hVS0VkHG540rO4U9zv\nAVepaknwpnBHvM3JEDYOSAf2AT4Km7cSGNSMdTbLwjXuFHUvtpIrQVmB0nJcXupo0nOpIoVUqgHI\nkgqKirZCz5y2bK4xxpg48R2MReQO4BogjfrezxUicreq3ux3Par6I3BqI3VW4KOHtaoeHqFsKjDV\nb3va0gKv89awpNWhM3oMh1jXf0XYltyN/Jr6IU3bCtYD/dqglcYYY+LN15GniFwF3Ag8hzvyHIm7\nhvsccKOI/KrNWtiJRe+8FeMUtacsNXTYdbnlpzbGmITl98j4UuDPqvqboLLvgPdFpAR3M4b7W7tx\nnVltrbK4KXdrClORlh+SZqWy2IKxMcYkKr/XZAcBb0WZ9xbteB22s1hVuJ1tFe6ab4Oe1DGGNQVU\nZ3YPma4tsSxcxhiTqPwG4y243M6RjPLmmyCBU9RA03pSezQrtEe1lFowNsaYROU3GL8K3C4i54pI\nKoCIpIjImcA06pOCGE/gtond2EZPCboFU3IadBvY6PLJXUJTYiaX2+8dY4xJVH6D8Q3AV8DTwHYR\n2QCU4e4p/DWuc5cJsjDa9eLuwyC58Uv1qbmhWbgyKiwYG2NMovKb9GObl0zjBOAQ3DjjAuB94J2m\n3LhhR7HAG2M8NCm8J3XjnbcAsrqFBuOsqsZSdxtjjOms/N7P+DLgPVV9E3izzVvVyW0sLmdzibtn\nRcOe1I1fLwbIyg9NiZlTW2T5qY0xJkH5vZ/x74l8tyUTQXDnreaMMYaGR8b5FLG9sqbFbTPGGNPx\n+L1mvBgY3JYNSSSBzlsAQxsMa/IXjCU7tANXd7ZRUFIRpbYxxpjOzG8wvgW4WUT2aMvGJIrAkXEW\n5fSToCFJkgTdh/pbSVo25aTXTaZLFVu3+k4DbowxphPxm4HrOiAH+FJEVgDrcHdcClBVPayV29Zp\nBYLx4PBT1HmDICW94QKRiFCS3JWMmo11RaUF62FI/1ZqpTHGmI7CbzCuARa1ZUMSRUlFNasKtwMw\nrEGyj8YzbwXbnpoHQcF4u+WnNsaYhOR3aNPhbdyOhJGTnsJXtxzNorXFZH4w0920McBHTupg4fmp\nqyw/tTHGJKRGrxmLSJqIvOqNMzY+dM1M5cAh3dk7c2PoDJ+dtwLC81PXlGxqadOMMcZ0QH6HNh3p\np64Js+nb0OkmBmPNCu1RbfmpjTEmMfkNsB8BY9uyIQmnuhIKloeWNfE0dVJO6M0iUsosJaYxxiQi\nvx24rgFe8+5d/BoNe1OjqrWt3LbOrWAZaFCSjty+kN6lSatI6xqa+COt0oY2GWNMIvJ7ZPwNMAT4\nM65LUiVQFfSobJPWdWYtPEUNkNm1V8i05ac2xpjE5PfIeBphR8KmEZuWhE77zEkdLDu/T8h0l5qt\nLWmRMcaYDsrv0KapbdyOxLP5u9Bpn3drCpYTdrOIPIqoqK4hPSW5JS0zxhjTwTS5h7SI5IjIQBFJ\nbYsGJYxN4cG4aQk/AJJyQntT51t+amOMSUi+g7GInCgi84Ai4AdgD6/8cRE5q43a1znV1sDm70PL\nmnGamtRMtpNZPyk1bC20scbGGJNofAVjEfk58DqwGZenOvimusuBia3ftE5s60qoCTqCzeoO2d2j\n149hW3LXkOnSgvUtaZkxxpgOyO+R8a3Ak6p6NHBf2LwFwO5+Nygi/UXkZREpEpFiEZkhIgP8Lh+0\nnhtEREXkwwjzrhaRf4nIOq/O1Kauv0XCO2814xR1wPbUvJDpMstPbYwxCcdvMB4JvOg9D+9VXQj4\nOuwTkSxgFrAr7mj6XGAYMFtEsn22BREZDNwEbIxS5WKgF25MdPsLH9bUxGQfwSrS8kOmK4uivWRj\njDGdld+hTcVAjyjzBgF+L2ReDAwGRqjqUgARmQ98D1wC/Mnneh4GngdGEPk1jFLVWhFJAS71uc7W\nszn8yLgZ14s91Rnd3d731G6zYGyMMYnG75Hx/wNuEJFuQWUqIunAlcA7PtdzEvBJIBADqOpyXLrN\nk/2swOssNhq4IVqduGcDC+9J3YIjY80O+w203VJiGmNMovEbjG8CegPfAY/jTlVfD3wF9AOm+lzP\nKNw15nALgd0aW1hE8oB7gcmq2jFzQ6q2yrCmgPDhTSlldrMIY4xJNL6CsaquwB2NvgkcBdQAhwKf\nAAeo6lqf28vHXWMOVwDkRSgPNx1YAjzlc3u+icgkEZkrInM3bWrB8KGaSjjgEtj1RHdEnJkHuTs3\ne3WpuaEpMS0/tTHGJB6/14xR1dXARa2wzUhpNSVCWWgFkUOA84DRqtrqqTlV9VHgUYAxY8Y0f/0p\n6XDEzfXTtTUgjb68qDK7hWbhsvzUxhiTeHwH41ZSiDs6DpdH5CPmYH8F/gasDrp2nQIke9Nlqtrx\n0lMltSx1ZXZe6J2bLD+1McYknianw2yhhbjrxuF2AxY1suxIXM/owqDHQbj7LBcCl7VeMzuOLt1D\nbxbRVYuorrG7VRpjTCJp7yPjN4C7RWSwqv4AICKDcEH1+kaW/WmEsvuAZOCXwNII8zu91C6h14xd\nfupyenbNilOLjDHGtLb2DsaP4YZCvS4iU3DXj28HVuFOQwMgIgOBZcA0VZ0GoKpzwlcmIluBlPB5\nIjIGN/45cOS/m4j8wnv+tqpub72X1MZS0ighmxxKAUgWpbhwIz27Dopvu4wxxrSadj1NraqlwDhc\nj+hncYk7lgPjVLUkqKrgjnib274rgZeozxp2mjf9Ei4zV6dSnNwtZHrbFstPbYwxiaS9j4xR1R+B\nUxupswIfPaxV9fAo5ecD5ze5cR3U9pRuULOmbrpsqwVjY4xJJL6DsZcP+nRgAJARNltVtTWGPZkI\nKtLzIaifeGWxpcQ0xphE4isYi8jJuFO8SbibM4QPIWr1cb+mXnh+6hrLT22MMQnF75HxHcAc4GxV\ntbvbtzPNCs1PLdstJaYxxiQSv8F4MHCNBeL4CM9PnVxmN4swxphE4re38rf4vGexaX3h+anTKyw/\ntTHGJBK/wXgycKPXicu0s4xuoSkxMy0/tTHGJBS/p6mn4o6MF4vI97i7LAVTVT2sNRtm6uWEpcS0\n/NTGGJNY/AbjGty9jE0cdMkPvXNTNy2itlZJSmr+3aCMMcZ0HL6CcbTkGqZ9ZIRdM86TErZuL6Nb\njuWnNsaYRNDed20yzZGcQhE5IUVFmy0LlzHGJArfwVhE+ojI3SLyuYgsE5HPROSPItK78aVNSxUn\n54VMlxRaMDbGmEThKxiLyHDgK+BXQAnwGVAK/Br4SkSGtVkLDQDbU0KD8fbCDXFqiTHGmNbmtwPX\nH3AJGQ/wbuIA1N3q8D/e/PGt3jpTpyI9LyQJaVWxBWNjjEkUfk9T/xS4OTgQA6jqStywp5+2brNM\nuOqM0JwrNdssGZoxxiQKv8E4DdgWZd42b75pQ5oVlgCt1PJTG2NMovAbjL8CfikiIfVFRIDLvfmm\nDUlO6PCm5HLLT22MMYnC7zXjacCbuAxcLwLrgN7AacAw4IS2aZ4JCM9PnVZhwdgYYxKF36Qf74rI\nibhbKd4ECO4exl8AJ6rqf9quiQYgMyw/dZblpzbGmITh98gYVX0XeFdEsoA8oFBVt7dZy0yI7LzQ\n4dw5lp/aGGMShu9gHOAFYAvC7axLj51DprvVFqGquMv2xhhjOrOowVhEbgEeV9W13vNYVFVvb92m\nmWDZXXtQo0KyKAC5sp2SsjJysiw/tTHGdHaxjoynAu8Ca73nsShgwbgNSVIyRZJLPkV1ZcWb15Ez\nYEgcW2WMMaY1RB3apKpJqvpZ0PNYj+T2a/KOqzi5a8j0ti3r4tQSY4wxrclvbuoBIpIaZV6KiAxo\n3WaZSMLzU5dttZSYxhiTCPwm/VgO7BNl3l7efF9EpL+IvCwiRSJSLCIzmhPMReQGEVER+TDCvCRv\n/goRKReRr0Xk1KZuo6MpT8sPma4osmBsjDGJwG8wjtVlNxWo9bUSNyxqFrArMBE4F5c0ZLaIZPts\nCyIyGDfeeWOUKrfjrnM/CBwHfAK8JCLH+91GR9QgP3WJ5ac2xphEEKs3dTcg+FCsrxcEg2Xigqrf\nm+teDAwGRqjqUm8784HvgUuAP/lcz8PA88AIwl6DiPQCrgV+r6p3e8WzRWQo8HvgbZ/b6HA0uwcE\nx1/LT22MMQkh1pHxr4GluECpwMve8+DHfFwQfdTn9k4CPgkEYgBVXQ58BJzsZwUichYwGrghSpVj\ncDeueC6s/DlgDxHZxWdbO5yk7J4h08llFoyNMSYRxBra9BqwAneK+glcKsxlYXUqgEWqOt/n9kYB\nr0coX4jLcx2TiOQB9wKTVbUgSsKLUV67loaVL/T+7kYTrnF3JCkN8lNbSkxjjEkEUYOxqn4NfA0g\nIgq8qaotvTtBPhApghTgUmw2ZjqwBHiqkW1sVVWNsI3A/AZEZBIwCWDAgI7ZObxhfuqCKDWNMcZ0\nJr46cKnq060QiOtWF6Gs0ZyOInIIcB5wWYRAG76uJm9DVR9V1TGqOqZnz56xqsZNdl5oMO5i+amN\nMSYh+M5NLSK7AxfhOk1lhM1WVT3Cx2oKiXxkmkfkI+ZgfwX+Bqz2OpeBa3+yN12mqhV4R9kiImFB\nO3Dk3WkPJ3PD8lN3rS2KUtMYY0xn4jfpxwHAXNwwoWNwgW0wcDgwFB9Htp6FuGu64XYDFjWy7Ejg\nUlzQDjwOAsZ6zy8L2kY6EJ4ncjfvb2Pb6bC6dO1BVVCys2wpp3x7SRxbZIwxpjX4HWd8JzADF0gF\nuEhVBwFHAsm4zl1+vAGMDR4iJSKDcEH1jUaW/WmEx9fAAu/5y169d4FK4Oyw5c8BFni9tzulpOQk\ntkpuSFmRpcQ0xphOz+9p6j1x44kDp32TAVR1lojcAdwFHOBjPY8BVwKvi8gU6m8wsQp3GhoAERmI\n67k9TVWneduaE74yEdkKpATPU9WNInIvcIOIbAPmAROAcfgcPtWRFSd1o2dt/Rn9bVvWsVP/YXFs\nkTHGmJbyG4xTgVJVrRWRAqBP0LzvgN39rERVS0VkHG540rO4o+z3gKtUNfh8q+ACvt8j93A3ASW4\nsdK9vTaerqr/aub6OoztKd3ccb+nrNBSYhpjTGfnNxgvA/p6z+cDF4rIm970BfjPwIWq/gjEzBOt\nqivwcR1aVQ+PUl6DO3Xu9/R5p1Genh8SjCuLo2UENcYY01n4PfL8F66zFrjrx8cBxbiOU2fhP42l\naaHqjNDO6DXbLBgbY0xn5+vIWFWnBj2fKSJjcUe3WcC7qvqftmmeCVebFTYGerulxDTGmM7O9zjj\nYKr6JfBlK7fF+JCUY/mpjTEm0fgdZzxWRE6PMu80bxyyaQepXcLyU5d32hwmxhhjPH6vGd9F5GQd\n4JJx3NU6zTGNSQ/LT51ZZTeLMMaYzs5vMN4L+CTKvM9w45BNO8jJ7x0ybfmpjTGm8/MbjDNi1E0G\nslunOaYxXfL7hEznahHEvG+GMcaYjs5vMF4MnBRl3km4pBqmHXTrlkeFptZNZ1JJVfm2OLbIGGNM\nS/kNxo8AF4vIdBEZLiJZIjJMRKbj7uT0l7ZrogmWkpJMYXh+6s1r49QaY4wxrcHvOOPHRGQE8Bvg\n6uBZwL2q+mhbNM5EVpzUjd619beXLi3YQI/+u8axRcYYY1rC9zhjVb1WRB7G3ampO7AZmKmqP7RV\n40xkpSl5ISkxtxf6zkZqjDGmA2pS0g9VXYbLU23iqDwtz/JTG2NMAokajEVkALBOVau85zF5N4Aw\n7aAqo7u7J5XH8lMbY0znFuvIeAUwFjeOeAX19zKOJrl1mmQaldU9ZFJLLSWmMcZ0ZrGC8QXUn5K+\nkMaDsWkn0iA/9ZYoNY0xxnQGsYJxV+qPdmfhnbJu+yaZxqTmhqbETKuwYGyMMZ1ZrHHG9wKDvOfL\ngX3avDXGl/SuoTeLsPzUxhjTucUKxluBQCJkwU5TdxjZYfmpc6otP7UxxnRmsU5TfwQ8LSJfe9MP\ni0hxlLqqqke0btNMNLndQ4Nxt0B+apE4tcgYY0xLxDoyvhj4B1CLOypOAVKjPNLatpkmWF63PLZr\net10KtXUlhXFsUXGGGNaIuqRsapuAC4HEJFaYJKqftZeDTPRpacks5pcsthUV7atYB1ds7rFsVXG\nGGOay++NInYBvmrLhpim2ZYcGnhLCiwlpjHGdFZ+bxSxsq0bYpqmJKVbWH7qDfFrjDHGmBaJlQ6z\nBjhQVT/zTlPH6k2tquorsItIf9ywqaNwvbRnAlc1lk5TRAYC9wN7A72AUmAB8AdVfSes7i7AdNxN\nLVJxWcR+q6pz/bSxM6hIyw8JxvkL/gbFn/hbuEtv2OsMyBvUJm0zxhjTNLEC6DRgddDzFg9tEpEs\nXAKRCmCit847gNkisqeqlsZYPAd3p6gpXrtycZ3M3haRU1V1hreN7sCHwDbgEmA77raPs0Vkf1Vd\n3NLX0RFUpeeH5Kfuvukz2OT/kn71l/8g5Yr/QVp2G7TOGGNMU8TqwHVb0POprbS9i4HBwAhVXQog\nIvOB73GB808x2rMQuCi4TETewiUkuQCY4RVfBuwEHBa0jVnAD8BtwOmt9FriqiqnD7Qg8VZK0Qoq\nvv0P6Xue0nqNMsYY0yx+O3A1ICL5IrKviKQ3XrvOScAngSAJoKrLcWOaT25qG1S1GigCgtN0jgW+\nD9tGKfBf4EQRadJtIzuqwv5HU6A5LVrH+nlvt1JrjDHGtITf67xTgGxVvcGbPhR4E8gG1ojIEar6\nvY9VjQJej1C+EDjNZ1uScD8ieuCOtIcDvw6qUkPI1dQ6FUAmMAT4zs+2OrKfjN6L0z+8m70qvyJD\nIr3chvrLRi5NebNuusvq9y1ZiDHGdAB+jxLPAe4Jmv4j8LX39xbgduAMH+vJByIlUi4A8ny25Y/A\nNd7zEuAMVX0vaP53wFEi0l1Vt0BdAN8/qA2dXv/8LJ781cm8v+QnlFfV+FqmcGsRFXP/Tbq4Ewn5\n1Ruo2rSU1F7D2rKpxhhjGuE3GPfFXddFRHoC+wFHqOocEUnD9XL2K1JHsKYcmt0HvIDLm30e8HcR\n+YWqBg75HgF+BTwjIr/CdeC6CTdWGlxGsYYNEJkETAIYMGBAE5oTP/3zszhn7EDf9atrapn7xa6M\n5Zu6slVz32Tw8b9pi+YZY4zxye814xrqU14eCpTjrvMCbML/0WZhlLp5RD5ibkBVV6vqXFV9U1VP\nBz4B7g6a/wNwNrAvsBRYCxyIG04FsC7Keh9V1TGqOqZnz56RqnR6KclJbOp1cEhZ1XfvRaltjDGm\nvfgNxguBc0QkB7gQeD/o3sb9gY1NWM+oCOW7AYt8riPcXGBocIGqvoI7mt8NGKqq++KGRq1qbDxz\nosvf85iQ6f5Fn6PV/q45G2OMaRt+g/E03JCgIuAI4A9B844H5vlczxvAWBEZHCgQkUHAQd68JvGu\nBR8MLAufp6o1qrpYVZeJyM7ABODhpm4j0ew95iA2ade66SzKWfXNB3FskTHGGF/BWFX/DYzEBeRR\nqvp+0OwPCA3OsTwGrABeF5GTReQkXO/qVcBfA5VEZKCIVIvILUFlU0XkfhGZICKHicgE4F1cx6xb\ng+qlisi9IvJzERknIr/EHT0vJLQT2g4pOyON73PGhJRt+NKGOBljTDz5HnPrjQdeHqH8rxGqR1tH\nqYiMw12/fRbXces9XDrMoHxSCJBM6I+FecBVuF7bXYH1uB7dh6jqR0H1FBgGnAV0w2XregK4U1Xt\nfCzAkHEwv/5acde1/41jY4wxxohq41kuReRkIF9Vn/SmB+J6NO8O/Bs4PyyYdmpjxozRuXMTJo11\nAxvXrKTXY3vWTdeoUHD5Ynru1CeOrTLGmMQjIl+o6pjG6vm9ZjwFCO5i/CegH/Aornf11KY20MRP\nr74DWZ68S910sijfffJWHFtkjDE7Nr/BeAgwH0BEMnGdtq5W1WuAGwFLcNzJFPYOHeJU870NcTLG\nmHjxG4wzgDLv+U9w15r/401/B+zcyu0ybazH3seGTA/Z9hnbK6qi1DbGGNOW/AbjFbghROBu6PCF\nqhZ5071wQ55MJ9J/r3FU1OVxgX6ymS++TNzr5MYY05H5DcZ/BaaKyFzgcuBvQfMOpPkJO0ycSFoW\nq7rsHVK2+at34tQaY4zZsfkdZ/xn4HzgY+BCVX0saHYX4MnWb5ppa8nDjgiZzl//ETW1jfeuN8YY\n01rUaC4AACAASURBVLp8389YVZ9X1V+q6jNh5Zeo6rOt3zTT1vqNOT5kel9dwNcrNsSpNcYYs+Py\nHYxN4kntswfFyfV3rsyRchZ9PiuOLTLGmB2T72AsIpNE5EsR2S4iNeGPtmykaSMiFO18SGjZMgvG\nxhjT3nwFYxE5D3gA+Bw3zOlJ4DmgGHeThmlt1UDTtrrvFTrEaffyL/hhU8IkUzPGmE7B75HxVcBd\nwGXe9F9UdSIwGDf+eEsbtM20g6wRR4ZM7ynL+e/87+LUGmOM2TH5DcbDcHdnqvUeaQCqWgj8Dvh1\nm7TOtL0uO7ElZ3jdZJIoBfNnxrFBxhiz4/EbjMuAJHV3lViPOyIOKMEycHVqqcNDj453LvgfBaV2\ngytjjGkvfoPxN8BQ7/l/gRtF5EAR2Q93k4hv26Btpp3kjjo6ZPrgpG+YvdiGOBljTHvxG4wfBQJj\nYG4GcoAPgU+A4cA1rd80024GHEhVUnrdZF/ZwvyvLTWmMca0lxQ/lVT1xaDnS0VkFC4NZhbwP1Xd\n3EbtM+0hNYPynceSuvr9uqL0lXMorzqJjNTkODbMGGN2DM1K+qGqpao6U1XfsECcGLJHHhUyfYB+\nzcc/WCd5Y4xpD1GDsYgMaMqjPRttWl/S0NA81QcmLWL2glXNW1nldlj9BVRsa4WWGWNM4ot1mnoF\n0JS7Btj5zM6s10gqMnqRXr4RgCypYNPi/1JbO5qkJPG/nuK18PRJsOV7yMyDSXMgb1BbtNgYYxJG\nrGB8IU0LxqYzEyFl+BEw/x91RaPK57FgbRF79uvmbx3VFdS8cC7JW75302WFVL9/Dyk/f6ANGmyM\nMYkjajBW1afasR2mA0geGhqMD0n6hpmLNvgOxiWvX0vO2tBe2JUL/kXKSfdBkp04McaYaGJdMxYR\n+ZmI7B6jzh4i8rO2aZppd4MPD5ncQ5bzyYIlvhZd/d6j5HzzTIPyrOpCald+0gqNM8aYxBWrN/W5\nwD+A0hh1tgH/EJEzW7VVJj5yelLda4+6ySRRem3+lFUF22Mu9umHM+n5wY1R52/8/JVWa6IxxiSi\nWMH4HOBJVV0erYKqrgD+Bkxs5XaZOEkZFtqr+pCkb3gvRjauv8/6gn7/bxLpUhW1TvrSt0Gt+4Ex\nxkQTKxiPBv7jYx0zgTF+Nygi/UXkZREpEpFiEZnhZ2iUiAwUkddFZKWIlInIZhGZIyLHRag7QESe\nFpEfvfsvLxGRO0Qk2287d1hDxoVMHpI8n5mLGgbj6ppabnvtKwbM/iV9JXQ88uPdrqJCU+um8yrX\nUbV2ftu01xhjEkCsYNwFKPSxjkKvbqNEJAuYBeyKO5o+F3dHqNk+AmUOsBmYAhwPXIS7ScXbIjI+\naBvZuB8Ih+JSd54API5L2fmEn3bu0AaMpTYlo25yZylg0/JvKCqrP/Itrahm0rNfsNPcuzk4eWHI\n4osHnsN5V97K/2SvkPLVH72IMcaYyGIF483AQB/rGODV9eNi3B2ffq6qr6nq68BJ3nYuibWgqi5U\n1YtU9VlVne0t+3P4/+2deXyU1dXHv2eysoWALGGPyI4KKiAIiohL64KKe1t3rdtbq/a1damt+tra\nxb2tu93ErdYNUawiAoKooKIFEnYQJGGHhEDWOe8f90mYmcwkk2Qyk+V8P5/nM3Pvc5/7nDvbb+49\n597LJuDygKLjcQJ/jar+wyv7B+BR4BzvD4ERieQ0fNnHBmUdI18zd+U2APL3FHP+UwtJXzmda5Pf\nDiq3u+tohl7yCKnJPrb3Dt58Im31u41rt2EYRjOmJjGeT3S+4Mu8stEwBfhUVVdXZng+6QXAmVHW\nUYWqlgN7gECHZar3WBBSfDeuvXVYwaKVEjpU7U1xWr65gLP+soCSvOX8MeWpoDJlbbuTeemLkOSG\np/sfcw7leuDj1bN0HfvyoovMNgzDaG3UJMaPAJNF5GERSQ09KSIpIvIocALwcJT3Gw4sDZO/DBgW\nTQUi4hORZBHJEpG7cLtG/SWgyCxgFfB7ERkmIu1F5ATgp8CTqlpTdLgB1cR4rC+HuTmbOO/JTygq\n2MlTKQ/TTkqqzqsvhZSLpkH7blV5Rww5hK99wW/p2o9fbly7DcMwmikRxVhVF+L8rDcCm0Rkmoj8\nxjum4YaHbwB+pqrRTiTtTHg/9E4ObNFYG3/A9YTzgJ8DF6rqhwF2FwMTcG1bhpt+9SEwA/ifSJWK\nyI9FZLGILN62bVuUprRQug5GO/SsSraVEoaV57CvtIwHU57gEF9eUHH5/u+hz5igPJ9P2NE3eKg6\n3YaqDcMwwlLjrk2q+ggwCVgMnA3c7h1ne3mTVPXROt4z3ByXugwdPwKMBs4AZgIvisjpVRWJpAOv\nAN1wAWITgVuBCwjuQQcbpfq0qo5S1VFdu3atgzktEBEkzFD1dUnTOTnpi+CyI38Eo64IW83BEy4I\nSg8ozWFn/oaYmmoYhtESqHU/Y1WdB8wTER/QxcveoaoV9bjfLlzvOJRORBe5japuwvXKAWaIyBzg\nAVzPF1yU9fHAAFVd4+XNE5E9wNMi8qSqfl0P21sXh0yCJdOqkuclzeEgQnZh6jESTnsAJPx/qQED\nBpObNJAhFauq8lbOfYWxF/y8UUw2DMNorkS9n7Gq+lV1q3fUR4jBDRsPD5M/DFhezzoXAwMC0ocB\nuwKEuJLPvceh9bxP66L/8UHJrlKATwIGNdp0hgueh5Q2EasQEXbZULVhGEatRC3GMWI6MFZE+ldm\niEg2bjrS9LpW5vXWJwCBwpsPdBKRASHFj/Yev6vrfVol7bpAjxHhz4kPzv0rZNa+jXX2+OCh6uGl\n37Dxu82xsNAwDKPFEG8xfga3T/JbInKmiEwB3gI2AlVzZbzVtspF5FcBeXeLyGMicoGITBSRC4D3\ngDHArwPu8Xdc0Na7InKpiEwSkVtxQ9lf4KZRGdEQ4jeuYvKv3DB2FPQYMIKNSX2q0ilSQe68f8XC\nOsMwjBZDXMXYm1Z0ArASeB54AVgHnKCqewOKCpAUYt+XwKHAn3DLdP4BKAaOVdWqOTPeetljgSXA\nfcC7uMVGngZOUlV/Y7StRRJOjIdOgfE31ama3f1OCUqnr5mJ2lrVhmEYVYj9KFZn1KhRunjx4toL\ntnTKS+BPR8GejS7dZRBcPRvSolr9tIo9az6n4/MnVaX3ayrrrvwvw/pmxdJawzCMJoeIfKGqte7f\nEO9haqM5kZwGF74AQ8+AkT+ES2fUWYgBOvYfzY6kA9PF2kgpyz9+M5aWGoZhNGtMjI2a6TECLpgG\nZz0OHbrXrw4R9vQLjqpuu+Zd/H4blTEMwwATYyNO9Bx3flB6fMUiPl8TeZ9kwzCM1oSJsREX0vtP\nYG9SRlW6o+xj6Sc259gwDANMjI14kZRMYd+TgrLar5tJSXk914/x+2H1LNiwMAbGGYZhJBYTYyNu\ndB1zTlD6eF3E3Nx6DFVXlMNLF8K0c+Bv34OZv4iRhYZhGInBxNiIG8kDJlPiO7B8Zpbs4utPZ9e9\nolm/hlX/OZD+7En44u8NN9AwDCNBmBgb8SMlnaK+wSt3ZX77HoXFZdHXsfQ1WPjn6vnv3gqbbG64\nYRjNExNjI650OnJqUHoyi/jP0vzoLt6yHH0rwpbUFaXoKxfD3la+F7VhGM0SE2Mjrsigk6mQAzt3\n9vfls3jxJ7VfuH83FS/9ACnbV5VVpknBdRdupuLVy5xP2TAMoxlhYmzEl/SOFPeeEJTVbdP7bC0s\njnyN30/RK1eStHtdUPZd5ZfzdPlpQXlJG+azb+YvY2auYRhGPDAxNuJOu5FnB6VP9i1mxtd5Ectv\nnH4v7dbPCsp7qXwSizqfwXNpl7KwYljQubaLnyB/wQuxM9gwDKORMTE24s/gU1GkKnmobz0Lv/wq\nbNEFM1+k11ePBOUt8ffnvb638Pr14/n3DcfyUMfb2Kydg8p0/OBmlkQz/G0YhtEEMDE24k/7bpT2\nHBOU1WfLbNZtL6pKqyp/nT6bQz/9GT45sIb1ds1gxpDf88wVE+jYJoU+ndvy7A2n8mT3uynRA77o\nNpSQ+fblvPXp8sZvj2EYRgMxMTYSQtqhU4LSpyQt4q0l3wFQUl7BbS9/yrjFN9JRDgRsVagwf8Qf\nuPOik0hNPvDR7dgmhbuuuZgZvW4OqjNb8mn3zvU89H6u7Z9sGEaTxsTYSAxDTw9KjpYVzPtyObuK\nSrn42c8Yt/xehvo2BpXJPexWzpp6ESJCKClJPqZefSfLss4Kyj8x6SuY+0dufmVJ/ZfeNAzDaGRM\njI3E0Cmb8m6HViV9ogzcM5/vPTqP4Rtf5KykYH/vzuzTGH7OHTVWKSIMv/IpdnU6LCj/puTX2P3N\nO1z87OfsKiqNXRsMwzBiRHLtRQyjcUgeNgW2Lq1Kn+JbxPrCLO5MDY6ELu08iM4XPQ1hesTVSEmn\n02UvU/bEcaQU7wCc0D+a8hfO2NCTqU+U8NuzD6N9WuN+9Lt2SCOrY3qj1V9QXEZhcTlZGekk+aJ4\nXQzDaNKI+dKqM2rUKF282JZWbHS2LIMnjqlKlmgyBbSlqxRU5WlqB+THc6DLgLrVvW4e+s8zEfVX\nZeX4+zC19B7203giGcig7u2ZPLQ7Jw7txsg+nRokmqrKmm1FfJizhQ9zt/LFhl1U+JVObVOYNLgb\nk4d259hBXchIT4lhCwzDaCgi8oWqjqq1nIlxdUyM44Qq/seOwLdrXeQyF74EQ06tX/0LHoMP7grK\nerPiGG4quwGIb2+yc7tUjh/clclDohfN0nI/i9bv5MOcrXyYu4UNO/bVWD7ZJxzdvzOTh3Rn8tBu\n9DuoXazMN2qgsLiMFfmF5OQXsmF7EZltUxjaI4MhPTLo2TE9bIxDXdlVVMryvAKWbd5Dbl4hFaoM\n7NY+5vepC6rKtsIScvILyckrYGV+IeX+QLs60CuzTdztamqYGDcAE+M48v5d8Mlj4c8d93M44c76\n160Kr14Gy98Myn6y/Ay+9vevf71RsFJ7s0Z7Ek70axLNnUWlzFmxlQ9ztjJv5TYKS+q/tOeAbu2Z\nPMT1mo/sm0lykoWINAS/X9mwcx+5eQVVApSbX8DGnfsjXpORnsyQHhkMzerAkB4ZDMnqwOCsDrRN\nDe8mUVW+272fZZsLWLa5gOWbC1i+eQ+b99SwQl3AfYZ59xjSI4PB3TvQJjWpxuuipbisgtVb95Ib\n0O6cvEJ21hKD0SE9maFZTpiHRtH+loiJcQMwMY4jGz+H506qnj/gJPjBK+Br4I9JyV54djJsy21Y\nPfXgg4qjuL3sKrbTscZyA7q1Z1z/g8jJK+DLb3fhj/IrmZ7io7jMX3tBILNtCscP6soRfTvha6I+\n5iQRfAI+EXy+CM+rHgW/Kn51AuZXqFD1nisVfvBXpaHCryhUXZ8kglQ+9wU/94kLBvSJkLdnPzl5\nToBW5Beyv6zhEfkikH1QO4ZkOYHqnpHGqi17nfjmFbBnfx12MavlPgcf1K5KBDPbpdbp+sLiMnLz\nCsnNL2DNtiIqov1gRmFXZfuHZGXQMzOdDunJtE9LoX16Mu3Tkr10Mm1Tk5p9z9rEuAGYGMcRvx8e\nGgp7A3Zu6pQNV38EbTtHvKxObF8Nz0yCkoLay8aYwqRM7qq4ijeLj2xwXalJPo7u35kTh3bnhCHd\n6NExncUbdjE7dyuzcrawdltR7ZUYRjNCBCfOaclVQt0uLZm0ZB+pyT5Sk7zHZB8p3vO0gDx3Pokk\nH/jV/UHz+7XqeYVf0cr8kPMVqvTObMP5o/s0sA0mxvXGxDjOfHQ/zP2de57cBq76ALIOq/maupL7\nLrz8AyAxn/dth5zDPzKu5d3V++okml3ap3oBWt2YMLBrjVHg67Z7AV45W1m0fiflMerJNDVGSS4T\nk75hrb8H7/jHUkrsg9a6sYuzkubjQ3m94li20iliWZ9A/67tGZLVgQHd2rN9bwk5eYXk5hVQVBq5\nJ51MOSf7FjNANjPDP5a12jNyWZ8wsHsHhvd0Q9EpyT5y8wrIza/9Po1JeoqPwd0PDEEnJ/nIzS/w\netSF7G2Am6UpcPTBnXnlmnENqqPJirGI9AEeBk7COdRmATep6re1XNcPeAwYCXQDioClwO9VdWZA\nubuBX0eopkRVaw2lNTGOM+Wl8PnTsGsdHHU5ZB1a+zX1YfUs+OZVKKs5EKrB7FwLW5ZWz+/YB856\nnHUdjqpRNIf2yPB8vd0Y0TuzXsPKe/aXMW/lNmbnbuWjFVvZvS82Q5+JQznB9xXXJU9ntG9lVe4W\nzeTZ8lN5sWIyRbRp8F2yJY9rkmYwNelj0sQJSYmmMK3iRJ4sP4Oytl2DfKBDszIY2L096SnV3Sl+\nv/P/Ls8rqBruzckrYNPOQqbIAm5MfoNs3xYASjWJG8p+ygf+UbRLTWJYzwyG9+zIsB4ZDOvp7pGW\nHN5l4/crm3Z59/GEMCe/oNaAv7rSK7MNQ6t8v+41yD6oXcRZAqrOrpw851+ubP+GnftoLn3A0dmd\nePXaY2ovWANNUoxFpC3wNVAC/BLXTbkPaAscrqoRuwwiMhy4BZgDbAIygKuB04BzVPV1r1xvoHfI\n5e2A94A3VPX82uw0MTYaREU5LHgY5vwO/GF6BmOvh8m/gpQ2VaK5auteunZI44Qh3eiV2XBRCaS8\nws9XG3fz8cpt7Giii54onu/X74YHnb8XtKKMkQWzOWnny/QqXRvx+n2+9szNPJuPO53DvpTMar5m\n8XzEqnjDkAf8zRWq9Ny/ihN3vMARe+fhI7wfXpPbwOgrkfE3QfuudW+kvwKWvo5/zv34dq6pflqS\n2X7KE3QZc15M/PpFJeWu55xfwOqteyktjy6+oJJkn9C/q4uMHpzVgY5tYjMCUVRSzsotheTkFbJq\nayF79pVRWFLO3uJyikrdY2U6Fj76hnBUv068dl3LFOOfAg8Bg1V1tZd3MLAK+LmqPlTH+pKBdcAS\nVT2jhnIXA/8ETlfVd2qr18TYiAl5X8Pr18C2nOrnugyCs5+CXg33JbdIyvbDV9NcpP3uGgfNgklu\nA0deAsf8BDJr8fWpwvr5MP9hWPNh9PdIaQtjfgzH3AjtDqq9vN/vIvrn/A62r6i5rCTBOc/AoedE\nb08LprzCT1FJBYUlZez1BHpvSTml5X5KK/zuMfB5YF5AusKvXqCe+4PmAvXcH7Qk74+bBAT3VQbx\n9WqpPmMR+RBIV9XxIflzAVR1Yj3qXAqsVNWpNZSZBRwK9FbVWp0YJsZGzCgrho/ug0/+TDV/tSTB\nxJ/DsT+DJFusA4DiPbDoWfj0CSjaFrlcn6Mh/7+RXQ6+ZDjsPBh/E3QbEnzO74eV78H8h2DTosj3\nyOznRjYKvgt/PrU9HH0tjLshfLCh3w+5bzsR3hph9zBfsusxB342xAdnPQkjLohsm9FsaKpinA+8\nparXhOQ/DpynqrWO/YiID7emdhfcMPVdwPdVNexfW2/YegPwiKr+LBo7TYyNmLN+Abx5bfheXs8j\nXS+566D429VUKNwCnz0Bi56LHPUuPhh2Fky4GXocDvt2wmdPwWdPQvHuyHUPPg2OvQV6jIClr8H8\nR8KPVlTSbbi7x/CznRh/+U/4+MHgiP9A0jKc62HsddAm0/W4V7zrAhO3/DdCW5Jg5EVw3K2wYSG8\ndT1o4DCywJl/hiN+FNnO1oYqlBe7Py9p7RNtTdQ0VTEuBR5S1dtC8u8DblPVWmeCi8gDQKWo7gUu\nrfQXRyh/O/BbYISqflNDuR8DPwbo27fvURs2bKjNFMOoG8UF8J874Kvnq59LTnc//q2xh1yyF3Lf\ngYqS8OeTUmHERTD+p3DQIeGv//IfbvShcHPk+6Rn1izafcY60R54cvV10Mv2wxd/h48fgqKtEerv\nCEddBmvnQt6S8GXEB4df4EQ4sC3fvApvXAMa4iM9/REYdXlkm6Ph288gZ3rdp/altIOBJ8EhJ0S3\nLnxd2bjIDd8XbXOvb3mxe6z2fL8bYSoPWFwlPdO9fp0PCXjs7x7bZMbe1gbQlMX4QVW9PST/N8Av\nohTj3kCWd1wCTAHOVdUZEcrnAPtVNWrnnPWMjUZlxUyY/pOah2ENNww86nIYewNk9Ki9fHkJfPMv\nWPAI7Fgd/X0GngwTboF+UUxhKd0Hi59zvet926O/BwKHnQsTfwFdBoYvsuwNeO2q6kF/pz4AY66u\nw708Ni6COb+FNbPrfm0gvUfDpDug/6TYiPKmL5xdq2c1vK5wtO1SXaA79HB/hOpKWnvoPrxB5jRV\nMd4CvNmQYeowdc4BslR1SJhzY4DPcFOnHo22ThNjo9Ep2g4zboKctxNtSdOj7UFw9HUw5ipoE3l+\nb0T8Fe51nf9wzT3U4VNhwk31m9NeshcWPQMLHoX9u2ouO3yqE+FQ33U4ct6GVy8Hf8hUtFPuh3HX\nR2fbd1+4IfLVH0RXPlr6jnOifPBx9bt+8xKYc7/z1zcX+o6DKxpmb1MV49lAqqpOCMmf49lSnwCu\nB3BiW61XLSJ/wQ0991TVqLshJsZGXFB1Pbl3b4WSPYm2JvF07OOioI+4GFLbNrw+VVj7kRPldfNc\nXlIaHPFDFwnd+eCG36Ok0PmtP/lT9SHwoVPg+Nvq3rNa8R7862KoCJmGdtK9bqg+EnlfOxFeOTNy\nmViQfSwcfztkj6+9LEDeNy6IbUWtE1lqJykN0OqvTWPRgsX4JuABYJCqrvXysnFTm25T1QfrWJ8P\n+ATopKqDQ86lAnnAfFU9sy71mhgbcWX/LljzUUKW62wyZPSG/hMbz2eevxS2r4R+46FD99jXX7zH\nifKKmW6I9JgbXZBZfVk9C17+ofOdBnLCL52/OZD8pa7HmRvWU+fIPtbFJES71rsqrHrfBaJF4uCJ\nMOlO6Ht0+PNblju7cqZHrqPvMS6QLa2Dm5aWku6mjiWnQ0obd1TmJ6c7+/1+FxuwYzXsWOMW2dmx\nBnaugZ3rqo8qNIQWLMbtcIt+7OfAoh//B3TALfqx1yvXD1gD3Kuq93p5dwOdgQVAPs5nfCVwIvAD\nVX055F5TgdcIWBAkWkyMDcNIOGvnwIsXBgcuAUy8zfW4t+U6sVv+VuQ6Gjq0/N0Xrle76v3IZQ6Z\n7O7R29ObbSvcNcveIOLys73HeH7o42MbHOavgD0bnThXCvSONe7PUn3IOhROf7hBJjVJMQYQkb4E\nL4f5IW6YeX1AmWzcYh73qOrdXt4U4CbcfOGOOEH+Grcc5oIw93kLmAD0UNU6jWmYGBuG0SRYPx9e\nOB/KQhYn7D0aNi0mstiNdr3W/sfHRuyiCQYbeLKLKP/vvyPb1esoJ8KHTG6cCO0mSJMV4+aAibFh\nGE2Gbz+FaedCaWHtZXse6UR4QCOJ3YaFTpQrffDR0mOEsyvctLEWjolxAzAxNgyjSbFxEUybGjmu\nIOtwJ3aDTomP2K2fDx/9FjZUG5QMpvthMOl2GHxqqxPhSkyMG4CJsWEYTY7vvoTnzwr2f3Y/1EU2\nDzkt/mKnCuvmOlHe+Fnwua5DnQgPOQN89Zjf24KIVoxrXWTDMAzDaAL0OhKunOWGiSvK4PDzEyt2\nIs4nffBE50te9JwLNjviYrdsaSsX4bpiPeMwWM/YMAzDiAXR9oztr4thGIZhJBgTY8MwDMNIMCbG\nhmEYhpFgTIwNwzAMI8GYGBuGYRhGgjExNgzDMIwEY2JsGIZhGAnGxNgwDMMwEoyJsWEYhmEkGFuB\nKwwisg3YEOZUF2B7nM1pKljbWy+tuf3W9tZLrNrfT1W71lbIxLgOiMjiaJY1a4lY21tn26F1t9/a\n3jrbDvFvvw1TG4ZhGEaCMTE2DMMwjARjYlw3nk60AQnE2t56ac3tt7a3XuLafvMZG4ZhGEaCsZ6x\nYRiGYSQYE2PDMAzDSDAmxjUgIn1E5N8iskdECkTkdRHpm2i74oGIHC8iGubYnWjbYo2I9BaRP4nI\nQhHZ57UzO0y5dBH5o4jkich+r/xx8bc4dtSh7eE+CyoiI+NvdWwQkXNF5DUR2eC9nytE5H4R6RBS\nrpOIPCsi20WkSERmichhibI7FkTTdhHJruF9z0yk/Q1FRE4Rkdkiki8iJSKySUT+JSLDQsrFTQOS\nG6PSloCItAVmAyXApYAC9wEficjhqlqUSPviyI3AooB0eaIMaUQGAOcDXwAfAydHKPcccBpwK7AW\nuAH4j4iMU9Ul8TC0EYi27QB/B54KyVvZOGbFhf8FvgXuADYBRwB3A5NE5BhV9YuIANOBg4GfALuA\n23G/AyNVdVNCLG84tbY9oOz9uNcgkMJ4GNmIdMZ95h8HtgF9gduAT0XkMFXdEHcNUFU7whzAT4EK\nYEBA3sE4Mbol0fbFof3Hex++ExNtSxza6gt4fpXX7uyQMiO8/MsD8pKBFcD0RLehMdvunVPgvkTb\nG+O2dw2Td4nX1hO89JleelJAmY7ATuCxRLehkdue7aWvSrS9cXpNBnvt/ZmXjqsG2DB1ZKYAn6rq\n6soMVV0HLMB9QY0Wggb3AiIxBSgDXgm4rhx4GThFRNIaybxGJcq2t0hUdVuY7MpRoF7e4xRgs6p+\nFHDdHuBtmvHvQJRtb23s8B7LvMe4aoCJcWSGA0vD5C8DhoXJb6m8ICIVIrJDRF5sLT7zMAwH1qnq\nvpD8ZUAqbri3pXOd51/b5/nbjk20QY3ARO8xx3us6Xegr4i0j4tV8SG07ZXcLyLlnt90enP3lwci\nIkkikioiA3EumHzcH2yIswaYzzgynXH+oVB2Ap3ibEsi2AM8CMwFCnA+pTuAhSJyhKpuTaRxCaCm\nz0Pl+ZbMNGAGsBnoh/ObzxaRk1R1TiINixUi0gu4F5ilqou97M7A+jDFK9/3TsDexreucYnQ9hKc\nQL2P86sOwf0GfCIiY1Q1VLSbI58BR3nPV+OG6Ct/2+KqASbGNRNuRRSJuxUJQFW/Ar4KyJorIvOA\nz3FBXb9MiGGJQ2jdn4eLA5Ifi8hbuF7DfcCExFgVO7we7ls4f+Dlgado4e97pLarah5wbUDRJdvp\nxQAACJdJREFUj0XkPVzP8E7gR/G0s5G4GMgA+uOC2j4QkQmqut47H7f33oapI7OL8L2dToT/t9Ti\nUdUvcdGzoxNtSwLYSeTPQ+X5VoOqFgLv0AI+CyKSjosW7g+cosER0rW97836t6CWtldDVTcC82kB\n7zuAquao6meq+hIwGWiPi6qGOGuAiXFkluF8BqEMA5bH2ZamRKSeQktnGXCwN90hkGFAKW6Iq7XR\n7D8LIpICvAaMAU5V1f+GFKnpd+BbVW22Q9RRtD3ipTTz9z0cqrob9z2ujP+IqwaYGEdmOjBWRPpX\nZniLIYyn+py7VoGIjAIG4fwsrY3pQApwXmWGiCQDFwDvq2pJogxLBCKSgZtz3Ww/CyLiA17A9YjO\nVNVPwxSbDvQSkYkB12UAZ9CMfweibHu46/rifgOb7fseCRHpjvOLr/Gy4qoBtlFEBESkHfA1sB/n\nH1Xg/4AOwOHN+R9xNIjIC8A64EtgNy6A63ZgH3Ckqm5PoHkxR0TO9Z5OxvnJrscFrWxT1blemZeB\nU3DBS+uA64DTgWO8IfxmSW1tF5H/xc3B/IgDAVyVeZNV9eP4W91wROQJXHt/gwtOC2STqm7yRGs+\n0Af3vlcu+nE4MMIbtm12RNn2B3EdtoW4z8NgXNs7Aker6oo4mhxTROQN3G/bN7gA1UHAzUAWMEZV\nV8ZdAxI90bopH7hVWV7z3qxC4E3CLIjQEg/cl+4bXFR1GbARt6VYj0Tb1kjt1QjHnIAybYCHcNMf\ninG9g+MTbXtjtx3XC1wAbPc+CztwPYMxiba9ge1eX0Pb7w4o1xn4K85/vA/4ECfECW9DY7YduAI3\n93gXLrgrH3gRGJxo+2PQ/l/gVuDa7b2nK3CR49kh5eKmAdYzNgzDMIwEYz5jwzAMw0gwJsaGYRiG\nkWBMjA3DMAwjwZgYG4ZhGEaCMTE2DMMwjARjYmwYhmEYCcbE2DDigIhcIiIbAtI5InJdjO8xTkQ+\nE5EiEVERGRmh3N0iogHpTC/vyFjaUxdEZKRnQ7W1gL223J0AswwjbpgYG0Z8OAq3yEDlLjmDKtMx\n5DncTmxnAONwm3qE41nvfCWZwK+BhIkxMNKzIdzC/ONwNhtGi8W2UDSM+HAUMDPguR+3wllM8JZt\nHAz8RlVn11RW3c48Ne7OEwN7BEhR1dKG1qVRrptsGM0Z6xkbRiPjCeVI3Fq44MR4uaoWR3l9hoj8\nWUQ2i0iJiKwQkZs9wUNELgMqcN/nu7xh3fU11Fc1TO0tfL/OO/WMd616dVaWnyoin4rIPhHZLSKv\nehsGBNa5XkSmicgVIpKL28nqNO/cPSLypYjsEZHtIjJbRMYGXHsZ8DcvuSrAhmzvfLVhahH5nogs\nFJH9Xr1visjgkDJzRGS+iJzo3X+fiCwVkbNCyg0SkTdEZKuIFIvIt14brbNixA0TY8NoJDyBUpxQ\ntgPe9dIPAoeHik6EOny4fYMv9647A3gPt0b2b7xi7wATvOfP4YZ1z47SzDxgqvf8fu/acV6diMi1\nuLV5lwPnAtcAhwJzRaRDSF2TgFuAe4DvcaDn3wt4GDgLuAzYCswTkcMD7L/Pe35egA154QwWke95\n1+zF7Zp1nWfTfBHpFVL8EOBR3Os11avz3yIyIKDMDM/G63AbgdwGlGC/j0Y8SfSC3XbY0VIP3L6n\nI3FCsMx7PhK36PzNAenUGuo4Hbd4/2Uh+c/iBKOLl04mZIODGuq82331q9LZ3rVXhZRrj9so5K8h\n+dm4nu9NAXnrcQvuZ9Vy7yTP1hXAowH5l3k2DAhzTejGDYuBVUByQN7BuE0sHgrIm+PlDQzI64b7\nc3SHl+7i1T8l0Z8XO1r3Yf/8DKORUNXlqroEt/3eHO95EW4LtldVdYl31ORXPQ7nX34pJH8akEpw\nIFasGQdkAC+ISHLlgfM353q2BfKpquaHVuINE38kIjtwu/+U4QLYBoeWrQ1vW7sjgVdUtbwyX1XX\n4XaWmhhyySpVXRVQbiuuZ145zL4DWAv8TkSuFpGBdbXJMGKBibFhNAIikhQgXuOBhd7zY4HvgHzv\nvNRSVWdgp6qWhOTnB5xvLLp5j7NwAhp4HAYcFFK+2rCyN13qXdyQ8pXAWGA0bp/Y9HrY1AmQcPfC\nvSahr8fOMOVKKu+tqgqchOtt3w+sFJG1sZ52Zhi1YQEKhtE4fEhwL+1576ikzHuchBtOjcROoLOI\npIb0oLO8xx0NtLMmKuu+DDfMHkphSDrcfqzn4HrDU1W1ss2ISCfcXrJ1ZZd3n6ww57Kox+uhqmuB\nS7w/RiOA/wEeF5H1qjqz5qsNIzZYz9gwGodrcD3AB4DV3vPRwDbglwHp2uYaz8V9T88Lyf8hzm8b\ni2k/lb3uNiH5n+AEd4CqLg5zrIii7rY4H23gIiMncGCYuDYbglDVItxrdp6IJAXU2Q84Bvd61Qt1\nLMEFoYELCjOMuGA9Y8NoBCqFSkTuAt5R1cXe1JsuwHPhfKsRmAnMB54Uka64HuqpwFXA/aq6PQbm\nbsH1KC8UkW9wfu11qrpDRG4F/uLdeyYuoKsXrtc/R1VfrKXu94CbgL+LyN9wvuK7cEP1gSz3Hm8Q\nkX/gRg6+ieBPvwsXTT1DRB7HBZrd49n2YB3ajRfR/SjwCu5PUxJuJKAcqHG+tmHEEusZG0YjISKp\nwGScIAF8H/iqDkKMqvpx83X/AfwCJ0Kn4Xpvd8bCTu8eV+H8sbOARbgpVKjqU8AUXLDV8zhBvgf3\nR35JFHX/B7gR5zefAVwBXIITvsByX+OivM/A/flYBPSMUOd7uNcgE/gX8CSQA0xQ1c3RttsjH/gW\n93pOxwXK9QROV9VYr5BmGBERF79gGIZhGEaisJ6xYRiGYSQYE2PDMAzDSDAmxoZhGIaRYEyMDcMw\nDCPBmBgbhmEYRoIxMTYMwzCMBGNibBiGYRgJxsTYMAzDMBLM/wMEiuMew9P8FAAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f161c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** ii: From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?\n",
    "\n",
    "There does not appear to be much overfitting. If overfitting were going to occur, we'd expect the test error to rise as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
