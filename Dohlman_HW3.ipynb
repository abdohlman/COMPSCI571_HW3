{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Anders Dohlman <br />\n",
    "COMPSCI 571 <br />\n",
    "Spring 2018\n",
    "\n",
    "\n",
    "***\n",
    "## 1. Separability\n",
    "***\n",
    "\n",
    "The convex hull of $\\{x_n\\}$ is the set of points $x$ given by \n",
    "\n",
    "$$ \\boldsymbol x = \\sum_n \\alpha_n x_n $$\n",
    "\n",
    "where $\\alpha_n \\geq 0 $ and $ \\sum_n \\alpha_n = 1 $. We want to show that if $\\{x_n\\}$ is linearly separable from some second set of points $\\{x_m'\\}$, then their convex hulls do not intersect. Let $w$ be the vector satisfying linear separability, that is, $w$ satisfies $ w^T x_n + w_0 > 0$ for all $x_n$ and $w^T x_m' + w_0 < 0$ for all $x_m'$. \n",
    "\n",
    "For the points on the convex hull of $\\{x_n\\}$, we have \n",
    "\n",
    "$$ f(\\boldsymbol x) = w^T \\boldsymbol x + w_0 = w^T \\left( \\sum_n \\alpha_n x_n \\right) + w_0 $$\n",
    "\n",
    "Since $\\sum_n \\alpha_n = 1$, we can write\n",
    "\n",
    "$$ f(\\boldsymbol x) = \\sum_n \\alpha_n \\left( w^T x_n + w_0 \\right) $$\n",
    "\n",
    "Similarly, for points on the convex hull of $\\{x_m'\\}$,\n",
    "\n",
    "$$ f(\\boldsymbol x') = \\sum_n \\alpha_m' \\left( w^T x_m' + w_0 \\right) $$\n",
    "\n",
    "For the sake of contradiction, assume that $\\{x_n\\}$ are linearly $\\{x_m'\\}$ and the convex hulls of $\\{x_n\\}$ and $\\{x_m'\\}$ intersect. Then there must exists some point $\\boldsymbol x^*$ on the intersection of both convex hulls, satisfying\n",
    "\n",
    "$$ f(\\boldsymbol x^*) = \\sum_n \\alpha_n \\left( w^T x_n + w_0 \\right) = \\sum_n \\alpha_n' \\left( w^T x_m' + w_0 \\right) $$\n",
    "\n",
    "However, since $\\{x_n\\}$ and $\\{x_m'\\}$ are linearly separable, we have that $ w^T x_n + w_0 > 0$ for all $x_n$ and $w^T x_m' + w_0 < 0$ for all $x_m'$. Since $\\alpha_n, \\alpha_m' \\geq 0$, the equality above does not hold, and we must therefore conclude that if $\\{x_n\\}$ and $\\{x_m'\\}$ are linearly separable, their linear hulls do not intersect.\n",
    "\n",
    "\n",
    "***\n",
    "## 2. Logistic Regression and Gradient Descent\n",
    "***\n",
    "### (a)\n",
    "\n",
    "The logistic sigmoid function is defined $\\sigma(a) = \\frac{1}{1+e^{-a}}$. Taking the derivative, we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma'(a)\n",
    "&= \\frac{e^{-a}}{(1+e^{-a})^2} \\\\\n",
    "&= \\left( \\frac{1}{1+e^{-a}} \\right) \\left( \\frac{e^{-a}}{1+e^{-a}} \\right) \\\\\n",
    "&= \\left( \\frac{1}{1+e^{-a}} \\right) \\left( \\frac{1+e^{-a}}{1+e^{-a}} + \\frac{-1}{1+e^{-a}}\\right) \\\\\n",
    "&= \\left( \\frac{1}{1+e^{-a}} \\right) \\left( 1 - \\frac{1}{1+e^{-a}}\\right) \\\\\n",
    "&= \\sigma(a)(1 - \\sigma(a))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### (b)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_w(\\{(x^{(i)},y^{(i)})\\}_{i=1}^n)} {\\partial w_j}\n",
    "&= \\frac{\\partial}{\\partial w_j}\n",
    "\\left[ \\sum_{i=1}^{n} \\{ -y^{(i)}\\log[h_w(x^{(i)})] - (1-y^{(i)})\\log[1-h_w(x^{(i)})] \\}\\right] \\\\\n",
    "&= \\sum_{i=1}^{n} -y^{(i)} \\frac{\\partial}{\\partial w_j} \\log[h_w(x^{(i)})] \n",
    "- (1-y^{(i)}) \\frac{\\partial}{\\partial w_j} \\log[1-h_w(x^{(i)})] \\\\\n",
    "&= \\sum_{i=1}^{n} \\frac{-y^{(i)}}{h_w(x^{(i)})} \\frac{\\partial}{\\partial w_j} [h_w(x^{(i)})]\n",
    "- \\frac{1-y^{(i)}}{1-h_w(x^{(i)}} \\frac{\\partial}{\\partial w_j} [1-h_w(x^{(i)}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We showed in (a) that $\\sigma'(a) = \\sigma(a)(1 - \\sigma(a))$. Then,\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_j} [h_w(x^{(i)})] = \\frac{\\partial}{\\partial w_j} \\sigma(w^T x^{(i)}) =  (x^{(i)})\\sigma(w^T x^{(i)})(1 - \\sigma(w^T x^{(i)})) $$\n",
    "\n",
    "Now substituting for $h_w(x^{(i)})$, we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_w(\\{(x^{(i)},y^{(i)})\\}_{i=1}^n)} {\\partial w_j}\n",
    "&= \\sum_{i=1}^{n} \\frac{-y^{(i)}}{\\sigma(w^T x^{(i)})} \\left((x^{(i)})\\sigma(w^T x^{(i)})(1 - \\sigma(w^T x^{(i)})) \\right)\n",
    "- \\frac{1-y^{(i)}}{1-\\sigma(w^T x^{(i)})} \\left( -(x^{(i)})\\sigma(w^T x^{(i)})(1 - \\sigma(w^T x^{(i)})) \\right) \\\\\n",
    "&= \\sum_{i=1}^{n} -y^{(i)}x^{(i)}(1 - \\sigma(w^T x^{(i)}) + (1-y^{(i)})x^{(i)}\\sigma(w^T x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{n} -y^{(i)}x^{(i)} + y^{(i)}x^{(i)}\\sigma(w^T x^{(i)}) + x^{(i)}\\sigma(w^T x^{(i)}) -y^{(i)}x^{(i)}\\sigma(w^T x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{n} x^{(i)}(\\sigma(w^T x^{(i)}) - y^{(i)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### (c)\n",
    "\n",
    "We want to show that the cross entropy loss of logistic regression is convex. To do so, we must consider the Hessian matrix of $L_w(\\{(x^{(i)},y^{(i)})\\}_{i=1}^n)$, composed of second derivatives:\n",
    "\n",
    "$$\n",
    "H(L_w) =\n",
    "\\left[\\begin{matrix}\n",
    "\\frac{\\partial^2 L_w}{\\partial w_1^2} & \\frac{\\partial^2 L_w}{\\partial w_1 \\partial w_2} & \\dots & \\frac{\\partial^2 L_w}{\\partial w_1 \\partial w_n} \\\\\n",
    "\\frac{\\partial^2 L_w}{\\partial w_2 \\partial w_1} & \\frac{\\partial^2 L_w}{\\partial w_2^2 } & \\dots & \\frac{\\partial^2 L_w}{\\partial w_2 \\partial w_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 L_w}{\\partial w_n \\partial w_1} & \\frac{\\partial^2 L_w}{\\partial w_n \\partial w_2} & \\dots & \\frac{\\partial^2 L_w}{\\partial w_n^2}\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "If the diagonal of this matrix is positive for all values, then the cross entropy loss of logistic regression is convex. Using the first partial derivative calculated in (b), we solve for the second partial derivative,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2 L_w}{\\partial w_j^2}\n",
    "&= \\frac{\\partial }{\\partial w_j} \\left[ \\frac{\\partial L_w}{\\partial w_j} \\right] \\\\\n",
    "&= \\frac{\\partial}{\\partial w_j} \\left[ \\sum_{i=1}^{n} x^{(i)}(\\sigma(w^T x^{(i)}) - y^{(i)}) \\right] \\\\\n",
    "&= \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w_j} \\left[ x^{(i)}(\\sigma(w^T x^{(i)}) - y^{(i)}) \\right] \\\\\n",
    "&= \\sum_{i=1}^{n} x^{(i)} \\frac{\\partial}{\\partial w_j} \\sigma(w^T x^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{n} x^{(i)} \\left((x^{(i)}) \\sigma(w^T x^{(i)})(1 - \\sigma(w^T x^{(i)})) \\right) \\\\\n",
    "&= \\sum_{i=1}^{n} (x^{(i)})^2 \\sigma(w^T x^{(i)})(1 - \\sigma(w^T x^{(i)}))\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "Notice that $(x^{(i)})^2 \\geq 0$ for all $i$. And since $\\sigma(a) \\in (0,1)$ for all $a$, we have that $\\sigma(w^T x^{(i)}) > 0 $ and $1 - \\sigma(w^T x^{(i)}) > 0$ for all $i$. Therefore  the summation above is positive for all $(x^{(i)},y^{(i)})$, and consequently every element along the diagonal of $H(L_w)$ is positive. We can thus conclude that the cross entropy loss $L_w$ of logistic regression is convex.\n",
    "\n",
    "\n",
    "### (d)\n",
    "\n",
    "[Click here to view notebook](http://nbviewer.jupyter.org/github/abdohlman/COMPSCI571_HW3/blob/master/logistic-regression-2d.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "## 3. Boosting\n",
    "***\n",
    "\n",
    "### (a)\n",
    "\n",
    "By the weak learning assumption, the error of AdaBoost is bounded above by\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol 1_{y_i \\ne H(x_i)} \\leq e^{-2\\gamma_{WLA}^2 T} $$\n",
    "\n",
    "Thus as $T \\to \\infty$, the AdaBoost misclassification error decays to zero. We can therefore conclude that after some  number of steps $T$ the boosted model eventually classifies the training set perfectly.\n",
    "\n",
    "\n",
    "### (b)\n",
    "\n",
    "Suppose that instead of weighting all misclassifications equally when computing exponential loss, the objective loss function for AdaBoost was\n",
    "\n",
    "$$ R^{train}(\\boldsymbol\\lambda) = \\sum_{i=1}^{n} w_i e^{-(\\boldsymbol M \\boldsymbol\\lambda)_i} $$\n",
    "\n",
    "We are interested in the weighted version of AdaBoost under in this scenario.\n",
    "\n",
    "We begin with coordinate descent on the exponential loss $R^{train}(\\boldsymbol\\lambda)$. At each iteration, we choose the $j_t \\in \\boldsymbol\\lambda$ that maximizes the directional derivative. That is,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "j_t &\\in argmax_j \\left[\\left. -\\frac{\\partial R^{train}(\\boldsymbol\\lambda_t + \\alpha e_j)}{\\partial \\alpha} \\right|_{\\alpha = 0} \\right] \\\\\n",
    "&= {argmax}_j \\left[ -\\frac{\\partial}{\\partial \\alpha} \\left[ \\sum_{i=1}^{n} w_i e^{-(M(\\lambda_t + \\alpha e_j))_i} \\right]_{\\alpha = 0} \\right] \\\\\n",
    "&= {argmax}_j \\left[ -\\frac{\\partial}{\\partial \\alpha} \\left[ \\sum_{i=1}^{n} w_i e^{-(M\\lambda_t)_i - \\alpha M_{ij}))} \\right]_{\\alpha = 0} \\right] \\\\\n",
    "&= {argmax}_j \\left[ \\sum_{i=1}^{n} w_i M_{ij} e^{-(M\\lambda_t)_i} \\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using the descrete probability distribution $d_{t,w_i} = \\frac{w_i e^{-(M\\lambda_t)_i}}{Z_t} $ where $Z_t=\\sum_{i=1}^{n} w_i e^{-(M\\lambda_t)_i}$. Then, \n",
    "\n",
    "$$ j_t \\in argmax_j \\sum_{i=1}^{n} M_{ij} d_{t,w_i} = argmax_j(d_w^T M) $$\n",
    "\n",
    "Now that we have selected $j_t$, we do linesearch along the $j_t$ axis until we reach a minimum:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "0 &= \\left. \\frac{\\partial R^{train}(\\boldsymbol\\lambda_t + \\alpha e_j)}{\\partial \\alpha} \\right|_{\\alpha_t} \\\\\n",
    "&= -\\sum_{i=1}^{n} w_i e^{-(M\\lambda_t)_i - \\alpha M_{ij}))} \\\\\n",
    "&= \\sum_{i:M_{ij_t}=-1}^{n} w_i e^{-(M\\lambda_t)_i} e^{\\alpha_t} \n",
    "-\\sum_{i:M_{ij_t}=1}^{n} w_i e^{-(M\\lambda_t)_i} e^{-\\alpha_t}  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Substituting for $d_{t,w_i}$, we get\n",
    "\n",
    "$$ 0 = \\sum_{i:M_{ij_t}=-1}^{n} d_{t,w_i} e^{\\alpha_t} \n",
    "-\\sum_{i:M_{ij_t}=1}^{n} d_{t,w_i} e^{-\\alpha_t} $$\n",
    "\n",
    "Letting $d_w^- = \\sum_{i:M_{ij_t}=-1}^{n} d_{t,w_i}$ and $d_w^+ = \\sum_{i:M_{ij_t}=1}^{n} d_{t,w_i}$, we can simplify to write\n",
    "\n",
    "$$ 0 = d_w^- e^{\\alpha_t} - d_w^+ e^{-\\alpha_t} \\implies d_w^- e^{\\alpha_t} = d_w^+ e^{-\\alpha_t} \\implies e^{2\\alpha_t} = \\frac{d_w^+}{d_w^-} \\implies \\alpha_t = \\frac{1}{2}ln\\frac{d_w^+}{d_w^-} = \\frac{1}{2}ln\\frac{1 - d_w^-}{d_w^-}$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "#### Coordinate Descent Algorithm\n",
    "*For AdaBoost with* $ R^{train}(\\boldsymbol\\lambda) = \\sum_{i=1}^{n} w_i e^{-(\\boldsymbol M \\boldsymbol\\lambda)_i} $\n",
    "\n",
    "Let $d_{t,w_i} = 1/n$ for $i=1,...,n$ and $\\boldsymbol\\lambda_1 = 0$\n",
    "\n",
    "For $t = 1...T$:\n",
    "* $j_t \\in argmax_j(d_w^T M)$\n",
    "* $d_w^- = \\sum_{i:M_{ij_t}=-1}^{n} d_{t,w_i}$\n",
    "* $\\alpha_t = \\frac{1}{2}ln\\frac{1 - d_w^-}{d_w^-}$\n",
    "* $\\boldsymbol\\lambda_{t+1} = \\boldsymbol\\lambda_{t} + \\alpha_t e_{j_t}$\n",
    "* $d_{t+1,w_i} = w_i e^{-(M\\lambda_{t+1})_i}/Z_{t+1}$ where $Z_{t+1}=\\sum_{i=1}^{n} w_i e^{-(M\\lambda_{t+1})_i}$\n",
    "\n",
    "End.\n",
    "\n",
    "### (c)\n",
    "\n",
    "[Click here to view notebook](http://nbviewer.jupyter.org/github/abdohlman/COMPSCI571_HW3/blob/master/adaboost-3c.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
